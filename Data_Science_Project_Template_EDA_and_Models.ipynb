{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab5Rgjr109uK"
      },
      "source": [
        "Here is the template for data science project, this notebook focus exclusively on exploratory data analysis part:\n",
        "\n",
        "By Emma HONG, DSBA master student from CentraleSuperlec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpZxbmpe2HLH"
      },
      "source": [
        "# Ordinary EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8O6231604DT"
      },
      "outputs": [],
      "source": [
        "# import the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# for modeling\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as stats\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "#from skopt import BayesSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "import joblib\n",
        "\n",
        "# for scoring\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error # regression\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, f1_score, accuracy_score, hinge_loss\n",
        "\n",
        "# for visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.pylab import rcParams\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from statsmodels.graphics.tsaplots import plot_pacf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAZCGCQo1zKB"
      },
      "outputs": [],
      "source": [
        "# Connect to Google Drive for later use\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY4amQYNHerQ"
      },
      "outputs": [],
      "source": [
        "# Show the files in the root filefolder\n",
        "import os\n",
        "for dirname, _, filenames in os.walk(os.getcwd()):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aumjHb-pbVA2"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmApBFHt12OG"
      },
      "outputs": [],
      "source": [
        "# Read data from files\n",
        "path = ''\n",
        "headnum = 1 # Change it according to the real case\n",
        "separater = ',' # Sometimes it's ';'\n",
        "# CSV file\n",
        "df = pd.read_csv(path,sep=separater) # add this code when you don't want certain columns: usecols = lambda x: x not in ['', ''] \n",
        "\n",
        "# Json file\n",
        "df = pd.read_json(path)\n",
        "# If you want to turn json file into a \n",
        "key = '' # Column name for the key\n",
        "value = '' # Column name for the value\n",
        "dictionary = df.set_index(key)[value].to_dict()\n",
        "\n",
        "# Excel file\n",
        "df = pd.read_excel(path,header = headnum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX1FsWDlJCK2"
      },
      "outputs": [],
      "source": [
        "# Specifically, if you want to merge the dataframe based on same columns\n",
        "df = pd.concat([df1,df2], axis = 0) # add this when you only want to merge same columns: join='inner'\n",
        "# or use methods similar to sql\n",
        "df_merge = df.merge(df1, how ='left', right_on='right_id', left_on='left_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJN8FXvcJ8-4"
      },
      "outputs": [],
      "source": [
        "# Make a copy if the later processing will overwrite the dataframe\n",
        "df_copy = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xORKfnE13R_O"
      },
      "outputs": [],
      "source": [
        "# Here is the codes to download data from Kaggle\n",
        "! pip install kaggle\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json # Get your kaggle.json from Kaggle website in advance\n",
        "\n",
        "# Import a toy kaggle dataset to try the model interaction\n",
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "kagglepath = '' # The link of the target file page\n",
        "od.download(kagglepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt4uGa6o4kFc"
      },
      "outputs": [],
      "source": [
        "# Take the first look on the data\n",
        "df.head() # You can specify the rows to view here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZSh2YnB43do"
      },
      "outputs": [],
      "source": [
        "# To be more clearer, show the meta information of each column\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2QOUSRAGD0C"
      },
      "outputs": [],
      "source": [
        "# Generate descriptive statistics\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZDSQDBZ5GOx"
      },
      "outputs": [],
      "source": [
        "# If you want to adjust the format of some columns manually\n",
        "# Encode the time format column\n",
        "columnname = ''\n",
        "typename = % # object, int64, float64, bool, datetime64, timedelta, category\n",
        "df[columnname] = pd.to_datetime(df[columnname])\n",
        "# Change the type for a specific column\n",
        "df[columnname] = df[columnname].astype(typename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRT9Z1LUE-sz"
      },
      "outputs": [],
      "source": [
        "# number of rows for each column\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJH2U7xx4iPj"
      },
      "outputs": [],
      "source": [
        "# Check the number of rows contain na value\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3yW9pOJQRLj"
      },
      "outputs": [],
      "source": [
        "# Show the number of missing value and frequency\n",
        "\n",
        "missing_values = df.isna().sum()\n",
        "\n",
        "df_missing = pd.DataFrame(data = missing_values, columns = ['count'], index = dict(missing_values).keys())\n",
        "df_missing['frequency'] = missing_values / df.shape[0]\n",
        "df_missing = df_missing.sort_values(by = 'count', ascending = False)\n",
        "\n",
        "df_missing.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmxdlyCuKTP-"
      },
      "outputs": [],
      "source": [
        "# check if the dataframe has duplicated rows(can limit to specific columns if you want)\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHK2b1zYYnpW"
      },
      "source": [
        "## Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxcjrTeEFKck"
      },
      "outputs": [],
      "source": [
        "# Plot the distribution of the values in certain column (categorical columns)\n",
        "cat_features = ['','','']\n",
        "\n",
        "# Create a figure with subplots\n",
        "# May need to change the nrows and ncols to suit the specific case\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(cat_features), figsize=(len(cat_features)*6, 6))\n",
        "\n",
        "# Loop through each categorical column and plot its value counts on a subplot\n",
        "for i, columnname in enumerate(cat_features):\n",
        "    df[columnname].value_counts().plot(kind='bar', ax=axes[i]) #kind = 'barh' if you prefer horizontal one\n",
        "    axes[i].set_title(columnname)\n",
        "\n",
        "# Adjust spacing between subplots and show the figure\n",
        "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75JOiRbQugzR"
      },
      "outputs": [],
      "source": [
        "# Another way to plot the distribution of caterical data(The input df should only contain categorical columns)\n",
        "# Distribution graphs (histogram/bar graph) of column data\n",
        "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
        "    nunique = df.nunique()\n",
        "    nRow, nCol = df.shape\n",
        "    columnNames = list(df)\n",
        "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
        "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k') # white/black\n",
        "    for i in range(min(nCol, nGraphShown)):\n",
        "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
        "        columnDf = df.iloc[:, i]\n",
        "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
        "            # To increase the readibility, only show top 50 results here\n",
        "            valueCounts = columnDf.value_counts().head(50)\n",
        "            valueCounts.plot.bar()\n",
        "        else:\n",
        "            columnDf.hist()\n",
        "        plt.ylabel('counts')\n",
        "        plt.xticks(rotation = 90)\n",
        "        plt.title(f'{columnNames[i]} (column {i})')\n",
        "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2WYP1emb55V"
      },
      "outputs": [],
      "source": [
        "# Plot the distribution of the values in certain column (numerical columns)\n",
        "num_features = ['','','']\n",
        "df[list_num_features].hist(bins = 100, figsize = (30, 25));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcRWFGdxRd5W"
      },
      "outputs": [],
      "source": [
        "# Another way to show the variable distribution\n",
        "optimal_bins_target = round(math.log(df.shape[0], 2) + 1) # based on Sturge's Rule: Optimal Bins = ⌈log_2(n) + 1⌉\n",
        "df[columnname].hist(bins = optimal_bins_target)\n",
        "\n",
        "# Print the skewness and kurtosis value\n",
        "print(\"Skewness: %.2f\" % df['valeurfonc'].skew())\n",
        "print(\"Kurtosis: %.2f\" % df['valeurfonc'].kurt())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmskz2BhYTAq"
      },
      "outputs": [],
      "source": [
        "# check distribution of numerical value\n",
        "fig, axes = plt.subplots(1,3, figsize=(21,6))\n",
        "sns.distplot(df[columnname], ax=axes[0])\n",
        "sns.distplot(np.log1p(df[columnname]), ax=axes[1])\n",
        "axes[1].set_xlabel('log(1+price)')\n",
        "# QQ plot\n",
        "sm.qqplot(np.log1p(df[columnname]), stats.norm, fit=True, line='45', ax=axes[2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1m6IfMQZG9f"
      },
      "outputs": [],
      "source": [
        "# calculation about numerical column\n",
        "pd.set_option('display.float_format',lambda x : '%.2f' % x) # Cancel scientific notation\n",
        "\n",
        "groupcolumn = ''\n",
        "result = df.groupby(groupcolumn).agg({columnname: ['mean', 'min', 'max']})\n",
        "print(\"Mean, min, and max values of price_per_square grouped by groupcolumn\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dLtdX6kYeXH"
      },
      "source": [
        "## Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nofFQTiDGAwX"
      },
      "outputs": [],
      "source": [
        "# Relationship between numerical data\n",
        "numericallist = ['','','']\n",
        "df_num = df[numericallist]\n",
        "# Only Sample 500 to get the intuition, use these codes when the dataset has large volume\n",
        "df_numsome = df_num.sample(n=500)\n",
        "sns.pairplot(df_numsome)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VM1uYjoG2Tz"
      },
      "outputs": [],
      "source": [
        "# When the data distribution has extreme outliers\n",
        "for col in numericallist:\n",
        "    df_numsome[col] = np.log(df_numsome[col])\n",
        "sns.pairplot(df_numsome)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gbUeeV6HUfk"
      },
      "outputs": [],
      "source": [
        "# Define functions to remove the outliers\n",
        "def remove_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] > lower_bound) & (df[column] < upper_bound)]\n",
        "df_ro = remove_outliers(df,columnname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-TVRUDGIQsg"
      },
      "outputs": [],
      "source": [
        "# Show the distibution after removing the outliers\n",
        "\n",
        "def without_outlier(df, columnname):\n",
        "    # Calculate interquartile range of the prices\n",
        "    q25 = np.percentile(df[columnname].values, 25)\n",
        "    q75 = np.percentile(df[columnname].values, 75)\n",
        "    IQR = q75 - q25\n",
        "\n",
        "    # Calculate the outlier cutoff\n",
        "    lower_bound = q25 - IQR*1.5\n",
        "    upper_bound = q75 + IQR*1.5\n",
        "\n",
        "    # Identify outliers and remove them for the data visualization\n",
        "    fig = plt.figure(figsize = (11, 7))\n",
        "    ax = fig.gca()\n",
        "    valueinside = df.loc[(df[columnname] >= lower_bound) & (df[columnname] <= upper_bound)][columnname]\n",
        "    valueinside.hist(bins = 100, ax = ax)\n",
        "    plt.show()\n",
        "\n",
        "    # Display the Skewness and Kurtosis of the prices without the outliers\n",
        "    print(\"Skewness: %.2f\" % valueinside.skew())\n",
        "    print(\"Kurtosis: %.2f\" % valueinside.kurt())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oFv1BehbZFT"
      },
      "outputs": [],
      "source": [
        "# Correlation heatmap(prettier, show only half of the graph)\n",
        "# Change the style theme\n",
        "sns.set(style = \"white\")\n",
        "\n",
        "non_numeric_columns = ['','','']\n",
        "# Compute the correlation matrix\n",
        "corr = df.drop(non_numeric_columns, axis = 1).corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype = bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "fig, axes = plt.subplots(figsize = (11, 9))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap = True)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask = mask, cmap = cmap, center = 0,\n",
        "            square = True, linewidths = .5, cbar_kws = {\"shrink\": .5}); # cbar_kws: colorbar setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VYD110QPjY5"
      },
      "outputs": [],
      "source": [
        "# Show the top 15 correlated features to the dependent variable\n",
        "targetcolumn = ''\n",
        "correlated_features = corr[targetcolumn].sort_values(ascending = False)\n",
        "\n",
        "correlated_features[1:16]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOiDwLrmP7Wr"
      },
      "outputs": [],
      "source": [
        "# Store the top 15 in a new DataFrame\n",
        "\n",
        "top_15_correlated_features = correlated_features[1:16].index\n",
        "\n",
        "df_top15_correlated_features = df[top_15_correlated_features]\n",
        "\n",
        "df_top15_correlated_features.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs-q5_h2RE1P"
      },
      "outputs": [],
      "source": [
        "# Check the correlation between the top 15 features(They may be highly correlated between each other)\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr_top15 = df[top_15_correlated_features].corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask_top15 = np.triu(np.ones_like(corr_top15, dtype = bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "fig, axes = plt.subplots(figsize = (11, 9))\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr_top15, mask = mask_top15, cmap = cmap, center = 0,\n",
        "            square = True, linewidths = .5, cbar_kws = {\"shrink\": .5});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edzd-tflTt5O"
      },
      "source": [
        "## Dealing with time series data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc02zmM0Txlr"
      },
      "outputs": [],
      "source": [
        "datecolumn = ''\n",
        "valuecolumn = ''\n",
        "# The distribution of data by time ( For time series data)\n",
        "datecolumn = ''\n",
        "df[datecolumn] = pd.to_datetime(df[datecolumn], format='%m/%d/%Y') # The format of date can change\n",
        "df[datecolumn] = df[datecolumn].dt.strftime('%Y-%m-%d')\n",
        "df[datecolumn].value_counts().sort_index().plot(kind = 'line')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs6Yb4OyZvBV"
      },
      "outputs": [],
      "source": [
        "# For panel data\n",
        "# Creating another dataset so we can re-use the original if we need it\n",
        "df_ = df.copy()\n",
        "df_ = df.sort_values(by=[datecolumn])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJNMujg1Z5Gp"
      },
      "outputs": [],
      "source": [
        "# resample the data by month\n",
        "dfM = df_.resample('M').mean()\n",
        "# check the boxplot of average price by month\n",
        "dfM.loc[:,[valuecolumn]].boxplot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDWvRcVAZ8VC"
      },
      "outputs": [],
      "source": [
        "# show the price change with the time\n",
        "fig, ax = plt.subplots(figsize=(12,6))\n",
        "dfM[valuecolumn].plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGyg_pVvaJlg"
      },
      "outputs": [],
      "source": [
        "# filter other factor\n",
        "factor = ''\n",
        "xx = 10 # customized value\n",
        "df_xx = df[df[factor]==xx]\n",
        "df_xx_M = df_xx.resample('M').mean().dropna()\n",
        "df_xx_Y = df_xx.resample('Y').mean()\n",
        "fig, ax = plt.subplots(figsize=(12,6))\n",
        "df_xx_M[valuecolumn].plot()\n",
        "plt.title(\"average value change by month\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npLHOry8agqY"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,6))\n",
        "df_xx_Y[valuecolumn].plot()\n",
        "plt.title(\"average value change by year\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu16Ygpfav-A"
      },
      "outputs": [],
      "source": [
        "# Check seasonality\n",
        "y_m_avg = df_xx_M.groupby(datecolumn)[valuecolumn].mean()\n",
        "y_m_avg = y_m_avg.dropna()\n",
        "\n",
        "decomposition = sm.tsa.seasonal_decompose(y_m_avg, model='additive', period=6)\n",
        "\n",
        "#Gather the trend, seasonality, and residuals\n",
        "trend = decomposition.trend\n",
        "seasonal = decomposition.seasonal\n",
        "residual = decomposition.resid\n",
        "\n",
        "# Plot gathered statistics\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.subplot(411)\n",
        "plt.plot(y_m_avg, label='Original', color='blue')\n",
        "plt.legend(loc='best')\n",
        "plt.subplot(412)\n",
        "plt.plot(trend, label='Trend', color='blue')\n",
        "plt.legend(loc='best')\n",
        "plt.subplot(413)\n",
        "plt.plot(seasonal,label='Seasonality', color='blue')\n",
        "plt.legend(loc='best')\n",
        "plt.subplot(414)\n",
        "plt.plot(residual, label='Residuals', color='blue')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D77LVnjya1p1"
      },
      "outputs": [],
      "source": [
        "# Stationary check\n",
        "\n",
        "def stationarity_check(TS):\n",
        "    \n",
        "    # Import adfuller\n",
        "    from statsmodels.tsa.stattools import adfuller\n",
        "    \n",
        "    # Calculate rolling statistics\n",
        "    roll_mean = TS.rolling(window=8, center=False).mean()\n",
        "    roll_std = TS.rolling(window=8, center=False).std()\n",
        "    \n",
        "    # Perform the Dickey Fuller test\n",
        "    dftest = adfuller(TS) \n",
        "    \n",
        "    # Plot rolling statistics:\n",
        "    fig = plt.figure(figsize=(12,6))\n",
        "    orig = plt.plot(TS, color='blue',label='Original')\n",
        "    mean = plt.plot(roll_mean, color='red', label='Rolling Mean')\n",
        "    std = plt.plot(roll_std, color='black', label = 'Rolling Std')\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Rolling Mean & Standard Deviation')\n",
        "    plt.show(block=False)\n",
        "    \n",
        "    # Print Dickey-Fuller test results\n",
        "    print('Results of Dickey-Fuller Test: \\n')\n",
        "\n",
        "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', \n",
        "                                             '#Lags Used', 'Number of Observations Used'])\n",
        "    for key, value in dftest[4].items():\n",
        "        dfoutput['Critical Value (%s)'%key] = value\n",
        "    print(dfoutput)\n",
        "    \n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is_D3bvcg94e"
      },
      "outputs": [],
      "source": [
        "ts_log_decompose = residual\n",
        "ts_log_decompose.dropna(inplace=True)\n",
        "stationarity_check(ts_log_decompose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51TNkOxnhDPu"
      },
      "outputs": [],
      "source": [
        "# Check autocorrelation\n",
        "# plot the ACF and PACF\n",
        "rcParams['figure.figsize']=7,5\n",
        "plot_acf(y_m_avg); plt.xlim(0,24); plt.show()\n",
        "plot_pacf(y_m_avg); plt.xlim(0,24); plt.ylim(-1,1);plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpUPvhEV2LkH"
      },
      "source": [
        "## Using PySpark to deal with big data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVCyv9aVxJou"
      },
      "outputs": [],
      "source": [
        "# Configuration codes\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop2.tgz\n",
        "!tar zxvf spark-3.3.1-bin-hadoop2.tgz\n",
        "!pip install -q findspark\n",
        " \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setMaster(\"local\")\n",
        "sc = SparkContext(conf = conf)\n",
        "print(\"initialization successful!\")\n",
        "\n",
        "\n",
        "#import of the SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#inizialization of the Spark Session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Template\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "\n",
        "seed_value=0\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czOzOPpwyaHq"
      },
      "outputs": [],
      "source": [
        "filename = ''\n",
        "# Load the file into an RDD\n",
        "RDD = sc.textFile(filename)\n",
        "\n",
        "# Create RDD with files, use wholeTextFiles to distinguish the files\n",
        "def load_text(filename):\n",
        "    return sc.wholeTextFiles(filename)\n",
        "# Read the texts into RDD, you can read several files in a time\n",
        "texts = load_text(\"filefolder/*.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajbKYsv7FDaX"
      },
      "source": [
        "Data example: \n",
        "\n",
        "[('file:/content/the_office/33793.txt',\n",
        "  \"06x12 - Scott's Tots\\nMikela\\nMikela: We just want to say thanks.\\n\"),\n",
        " ('file:/content/the_office/4199.txt',\n",
        "  '02x08 - Performance Review\\nMichael\\nMichael: What do you say, Jan?\\n'),\n",
        " ('file:/content/the_office/11602.txt',\n",
        "  \"03x08 - The Merger\\nPam\\nPam: Oh, I'll just time him later.\\n\"),\n",
        " ('file:/content/the_office/6578.txt',\n",
        "  '02x18 - Take Your Daughter to Work Day\\nToby\\nToby: [to Sasha] Okay, tell them what you wanted to say.\\n'),\n",
        " ('file:/content/the_office/9020.txt',\n",
        "  '02x99 - Deleted Scenes from Season 2\\nMichael\\nMichael: I used to be in HR. I was a Hell raiser.\\n')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coqHM0-IzCOZ"
      },
      "outputs": [],
      "source": [
        "# Functions frequently used to deal with texts data\n",
        "\n",
        "import string\n",
        "import re\n",
        "from stop_words import get_stop_words\n",
        "\n",
        "# remove punctuations, only remove the punctuations on both sides\n",
        "def remove_punctuation(word):\n",
        "    punctuation = string.punctuation\n",
        "    word = word.strip(punctuation)\n",
        "    return word\n",
        "\n",
        "# remove punctuations\n",
        "def remove_non_letters(word):\n",
        "    if type(word) != str:\n",
        "        word = \"\"\n",
        "    regex = re.compile(r'[^\\w\\s]')\n",
        "    return regex.sub('', word)\n",
        "\n",
        "# remove the stop words\n",
        "def removestopwords(word):\n",
        "    stop_words = get_stop_words('english')\n",
        "    if word in stop_words:\n",
        "      filtered_word = \"\"\n",
        "    else:\n",
        "      filtered_word = word\n",
        "    return filtered_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDyZ1GBLzwXJ"
      },
      "outputs": [],
      "source": [
        "# Sample codes for conducting MapReduce method on RDDs\n",
        "# Use mapreduce to count the number of words in the whole set of dialogs\n",
        "# Map method can apply the same function to the RDD element wise\n",
        "diagnum = diag.map(lambda x: (x,1))\\\n",
        "              .reduceByKey(lambda x,y: x+y)\\\n",
        "              .sortBy(lambda f: f[1], ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnniDxFn0PfU"
      },
      "outputs": [],
      "source": [
        "# If you only want to apply the function on the value of a pair (x,y) within the RDD, use mapValues function\n",
        "# Use case: Find common words by character\n",
        "# Gather the dialogs under each character\n",
        "cha_diag = texts.map(lambda x: (x[1].split(\"\\n\")[1],dialog(x[1].split(\"\\n\")[2]).split(\" \")))\\\n",
        "                .reduceByKey(lambda x,y:x+y)\n",
        "# Use mapValues to only process the value to split the words and clean the words\n",
        "# The flatMap sentence is to spread the (character,word) and then make it easier to count the word frequency for each character\n",
        "rdd = cha_diag.flatMap(lambda x: [(x[0], y) for y in x[1]])\\\n",
        "         .mapValues(lambda word: remove_punctuation(word))\\\n",
        "         .mapValues(lambda word: word.lower())\\ # lower() method is useful to remove the influence of the capitalizartion of the words\n",
        "         .mapValues(lambda x: removestopwords(x))\\\n",
        "         .filter(lambda word: len(word[1]) > 0) # empty string is not useful\n",
        "# Do the mapreduce to get the result\n",
        "# sorted sentence is to sort the tuples by the word frequency\n",
        "rdd1 = rdd.map(lambda x: (x, 1))\\\n",
        "          .reduceByKey(lambda x, y: x+y)\\\n",
        "          .map(lambda x: (x[0][0],(x[0][1],x[1]))).groupByKey()\\\n",
        "          .map(lambda x: (x[0], sorted(x[1], key=lambda y: y[1], reverse=True))) \\\n",
        "          .map(lambda x: (x[0],x[1][:10]))\n",
        "\n",
        "# Here we only put 5 as examples\n",
        "print(\"The 10 most common words for each character is:\",rdd1.take(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxZwKr5N1oU9"
      },
      "outputs": [],
      "source": [
        "# Spark Dataframe, similar to pandas dataframe\n",
        "# Read the csv series\n",
        "dfs =spark.read.option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").csv(\"XXX.csv\", header=True)\n",
        "dfs.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGSaEWtx60Cu"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Example of data\n",
        "+---+------+-------------+--------------------+-------+-----+----------+--------+--------------+----------+---------------+--------------------+\n",
        "|_c0|Season| EpisodeTitle|               About|Ratings|Votes|Viewership|Duration|          Date|GuestStars|       Director|             Writers|\n",
        "+---+------+-------------+--------------------+-------+-----+----------+--------+--------------+----------+---------------+--------------------+\n",
        "|  0|   1.0|        Pilot|The premiere epis...|    7.5| 4936|      11.2|      23| 24 March 2005|      null|     Ken Kwapis|Ricky Gervais |St...|\n",
        "|  1|   1.0|Diversity Day|Michael's off col...|    8.3| 4801|         6|      23| 29 March 2005|      null|     Ken Kwapis|         B. J. Novak|\n",
        "|  2|   1.0|  Health Care|Michael leaves Dw...|    7.8| 4024|       5.8|      22|  5 April 2005|      null|Ken Whittingham|    Paul Lieberstein|\n",
        "|  3|   1.0| The Alliance|Just for a laugh,...|    8.1| 3915|       5.4|      23| 12 April 2005|      null|   Bryan Gordon|       Michael Schur|\n",
        "|  4|   1.0|   Basketball|Michael and his s...|    8.4| 4294|         5|      23| 19 April 2005|      null|   Greg Daniels|        Greg Daniels|\n",
        "+---+------+-------------+--------------------+-------+-----+----------+--------+--------------+----------+---------------+--------------------+\n",
        "only showing top 5 rows\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdKrV_z43ZBD"
      },
      "outputs": [],
      "source": [
        "# Show 5 value as example\n",
        "dfs_distinct = dfs.select(columnname).distinct()\n",
        "dfs_distinct.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7umL6BCp3lLJ"
      },
      "outputs": [],
      "source": [
        "# Find the number of episodes each director directed\n",
        "# Using parquet to read dataframe and use pandas function to count the number of episodes each director directed\n",
        "# Finally transform pandas dataframe into spark dataframe\n",
        "dfs = dfs.select(\"Director\",\"EpisodeTitle\")\n",
        "dfs.write.parquet(\"template/sample\",mode='overwrite')\n",
        "df = pd.read_parquet(\"template/sample\")\n",
        "# Use pandas dataframe to count\n",
        "pdf = df[\"Director\"].value_counts().sort_values(ascending=False)\n",
        "pdf1 = [(i,v) for i,v in pdf.iteritems()]\n",
        "\n",
        "# Finally tranform the pandas dataframe back to spark dataframe and print out the result\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "schema = StructType([ \\\n",
        "    StructField(\"Director\",StringType(),True), \\\n",
        "    StructField(\"number_of_episode\",IntegerType(),True),\n",
        "  ])\n",
        "rdd = sc.parallelize(pdf1)\n",
        "sdf = spark.createDataFrame(rdd,schema)\n",
        "print(\"The number of episodes each director directed: \")\n",
        "print(sdf.where(\"Director <> ''\").show(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwvkWPAe4WEG"
      },
      "outputs": [],
      "source": [
        "# Find the 10 most common words for each character\n",
        "import collections\n",
        "# Read the data in pandas dataframe\n",
        "df = pd.read_csv(\"XXX.csv\",sep=';')\n",
        "\n",
        "# Deal with the words\n",
        "df[\"words\"] = df[\"Field\"].apply(lambda x: dialog(x)).apply(lambda x: x.split(\" \"))\n",
        "df1 = df.explode(\"words\") # Spread the words in rows\n",
        "df1[\"words\"] = df1[\"words\"].apply(remove_punctuation) # Remove the punctuation in both sides\n",
        "df1[\"words\"] = df1[\"words\"].apply(lambda x: x.lower())# Convert the words to lowercase\n",
        "df1[\"words\"] = df1[\"words\"].apply(removestopwords)# Remove stopwords\n",
        "df1 = df1[df1[\"words\"] != \"\"] # Filter the empty elements\n",
        "# Count the word frequency by characters and save the top 10 common words\n",
        "top10_words = df1.groupby(\"Field1\")\n",
        "top10_words = top10_words.apply(lambda x: collections.Counter(x[\"words\"]).most_common(10))\n",
        "top10_words = pd.DataFrame(top10_words)\n",
        "\n",
        "print(\"The 10 most common words for each character are:\")\n",
        "# Here we take 5 characters as examples:\n",
        "top10_words.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-E6adJE74IdI"
      },
      "outputs": [],
      "source": [
        "# Find the 10 episodes having the most number of words\n",
        "from pyspark.sql.functions import explode, count,split,desc,lower,udf\n",
        "# Select the dialogs and episodes to process\n",
        "dfs = dfst.select(\"Field\",\"Text\")\n",
        "dfs = dfs.withColumnRenamed(\"Text\",\"episode\")\n",
        "# Count the words for each episode\n",
        "udf_dialog = udf(dialog)\n",
        "dfs = dfs.withColumn(\"dialogs\",udf_dialog(col(\"Field\"))) # extract the dialog\n",
        "dfs = dfs.withColumn(\"words\", explode(split(\"dialogs\", \" \"))) # spread the words\n",
        "word_counts = dfs.groupBy(\"episode\").agg(count(\"words\").alias(\"word_count\")).orderBy(desc(\"word_count\")) # Count the words by episode\n",
        "\n",
        "print(\"The 10 episodes having the most number of words are:\")\n",
        "# Print the result:\n",
        "word_counts.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsdE-z5uCSXs"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWBiQMH5CS04"
      },
      "outputs": [],
      "source": [
        "# Do One-hot encoding to '',''\n",
        "df_encoded = pd.get_dummies(df, columns=['',''])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5w9VhnVCYC-"
      },
      "outputs": [],
      "source": [
        "# Create scaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# indicate the numerical columns that need to be scaled\n",
        "numeric_cols = ['','']\n",
        "\n",
        "# log first(better change the column name, do it later)\n",
        "for col in numeric_cols:\n",
        "    df_encoded[col] = np.log(df_encoded[col]+1)\n",
        "\n",
        "# Scale data\n",
        "scaler.fit(df_encoded[numeric_cols])\n",
        "scaler = scaler.transform(df_encoded[numeric_cols])\n",
        "scaler = pd.DataFrame(scaler,columns=numeric_cols)\n",
        "df_encoded[numeric_cols] = scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESAMwOumCcJa"
      },
      "outputs": [],
      "source": [
        "df_encoded.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bya2P0j0CceF"
      },
      "outputs": [],
      "source": [
        "# drop useless columns\n",
        "keepcolumns = ['', '']\n",
        "train_df = df_encoded[keepcolumns]\n",
        "train_df = train_df.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnLBOGruChNI"
      },
      "outputs": [],
      "source": [
        "cols = train_df.columns.tolist()\n",
        "cols.remove('')\n",
        "X = train_df[cols]\n",
        "y = train_df[['']]\n",
        "# If PCA\n",
        "#pca=PCA(n_components='mle')\n",
        "#X = pca.fit_transform(X)\n",
        "#pca1=PCA(n_components=33)\n",
        "#test_df = pca1.fit_transform(test_df)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iwxuW3mHPa0"
      },
      "source": [
        "# Model training(including tuning part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgLjYmNxoNai"
      },
      "source": [
        "### Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yxJZiyeokIu"
      },
      "outputs": [],
      "source": [
        "# Example from https://www.datacamp.com/tutorial/feature-selection-python\n",
        "# load data\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = pd.read_csv(url, names=names)\n",
        "\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhRpy51SoMw7"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries first\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLxl1gF5rBQJ"
      },
      "source": [
        "**Filter Method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5JlUtJMoWBm",
        "outputId": "bf24fa94-0c94-4365-c7f0-e107b06e001a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'], dtype='object')\n",
            "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
            "[[148.    0.   33.6  50. ]\n",
            " [ 85.    0.   26.6  31. ]\n",
            " [183.    0.   23.3  32. ]\n",
            " [ 89.   94.   28.1  21. ]\n",
            " [137.  168.   43.1  33. ]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Feature extraction\n",
        "test = SelectKBest(score_func=chi2, k=4)\n",
        "fit = test.fit(X, Y)\n",
        "\n",
        "# Summarize scores\n",
        "np.set_printoptions(precision=3)\n",
        "print(dataframe.columns)\n",
        "print(fit.scores_)\n",
        "\n",
        "features = fit.transform(X)\n",
        "# Summarize selected features\n",
        "print(features[0:5,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LqX-W2gq7GM"
      },
      "source": [
        "**Wrapper Method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18yXagbypW4r"
      },
      "outputs": [],
      "source": [
        "# Import your necessary dependencies\n",
        "# Recursive Feature Elimination \n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8D7uxhTpllP",
        "outputId": "30148998-c725-4a1b-e866-9e209e1b6168"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num Features: 3\n",
            "Selected Features: [ True False False False False  True  True False]\n",
            "Feature Ranking: [1 2 4 5 6 1 1 3]\n"
          ]
        }
      ],
      "source": [
        "# Feature extraction\n",
        "model = LogisticRegression()\n",
        "rfe = RFE(estimator=model, n_features_to_select=3)\n",
        "fit = rfe.fit(X, Y)\n",
        "print(\"Num Features: %s\" % (fit.n_features_))\n",
        "print(\"Selected Features: %s\" % (fit.support_))\n",
        "print(\"Feature Ranking: %s\" % (fit.ranking_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DFQX0S6rd0V"
      },
      "source": [
        "**Embedded Methods**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In5IQxK1rfJO",
        "outputId": "13dbfd4c-ae59-4068-d922-e5d51aec5c87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ridge model: 0.021 * X0 + 0.006 * X1 + -0.002 * X2 + 0.0 * X3 + -0.0 * X4 + 0.013 * X5 + 0.145 * X6 + 0.003 * X7\n"
          ]
        }
      ],
      "source": [
        "# First things first\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X,Y)\n",
        "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
        "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
        "# A helper method for pretty-printing the coefficients\n",
        "def pretty_print_coefs(coefs, names = None, sort = False):\n",
        "    if names == None:\n",
        "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
        "    lst = zip(coefs, names)\n",
        "    if sort:\n",
        "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
        "    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
        "                                   for coef, name in lst)\n",
        "    \n",
        "print (\"Ridge model:\", pretty_print_coefs(ridge.coef_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qzszP9bdfdI"
      },
      "source": [
        "## Unsupervised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8gZY2V0Tfrx"
      },
      "source": [
        "### K-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAoiC7wMSd75"
      },
      "outputs": [],
      "source": [
        "# Unsupervised learning\n",
        "# Elbow method to determine the number k of clusters\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Initialisation of the list of values k\n",
        "number_clusters = range(1, 7)\n",
        "\n",
        "# Initialisation of an empty list 'distortions' which will contain the distortion values of each Kmeans models depending of the number k\n",
        "distortions = []\n",
        "\n",
        "# For each value of k, fit a different KMeans algorithm\n",
        "for k in number_clusters:\n",
        "    kmeans = KMeans(n_clusters = k, random_state = 42)\n",
        "    kmeans.fit(X_scaled)\n",
        "\n",
        "    # Compute the distortion of each constructed model\n",
        "    distortions.append(sum(np.min(cdist(X_scaled, kmeans.cluster_centers_, 'euclidean'), axis = 1)) / np.size(X_scaled, axis = 0))\n",
        "    \n",
        "# Create the distortions graph depending on the number of clusters\n",
        "plt.figure(figsize = (16, 9))\n",
        "plt.plot(number_clusters, distortions, 'gx-')\n",
        "plt.xlabel('Number of Clusters k')\n",
        "plt.ylabel('Distortion (WSS/TSS)')\n",
        "plt.title('Elbow method displaying the optimal number of clusters')\n",
        "plt.savefig('KMeans_distortions_graph.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_tjbdnZT_4k"
      },
      "source": [
        "Quote from : https://github.com/smazzanti/are_you_still_using_elbow_method/blob/main/are-you-still-using-elbow-method.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp6bJc7CUVTR"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics as sklearn_metrics\n",
        "\n",
        "def calinski_harabasz_score(X, labels):\n",
        "  \"\"\"Wrapper function of Scikit-learn's calinski_harabasz_score. The only difference is it doesn't throw an error where there is only one label.\"\"\"\n",
        "  \n",
        "  if len(set(labels)) == 1:\n",
        "    return float(\"NaN\")\n",
        "  else:\n",
        "    return sklearn_metrics.calinski_harabasz_score(X, labels)\n",
        "\n",
        "\n",
        "def bic_score(X: np.ndarray, labels: np.array):\n",
        "  \"\"\"\n",
        "  BIC score for the goodness of fit of clusters.\n",
        "  This Python function is translated from the Golang implementation by the author of the paper. \n",
        "  The original code is available here: https://github.com/bobhancock/goxmeans/blob/a78e909e374c6f97ddd04a239658c7c5b7365e5c/km.go#L778\n",
        "  \"\"\"\n",
        "    \n",
        "  n_points = len(labels)\n",
        "  n_clusters = len(set(labels))\n",
        "  n_dimensions = X.shape[1]\n",
        "\n",
        "  n_parameters = (n_clusters - 1) + (n_dimensions * n_clusters) + 1\n",
        "\n",
        "  loglikelihood = 0\n",
        "  for label_name in set(labels):\n",
        "    X_cluster = X[labels == label_name]\n",
        "    n_points_cluster = len(X_cluster)\n",
        "    centroid = np.mean(X_cluster, axis=0)\n",
        "    variance = np.sum((X_cluster - centroid) ** 2) / (len(X_cluster) - 1)\n",
        "    loglikelihood += \\\n",
        "      n_points_cluster * np.log(n_points_cluster) \\\n",
        "      - n_points_cluster * np.log(n_points) \\\n",
        "      - n_points_cluster * n_dimensions / 2 * np.log(2 * math.pi * variance) \\\n",
        "      - (n_points_cluster - 1) / 2\n",
        "    \n",
        "  bic = loglikelihood - (n_parameters / 2) * np.log(n_points)\n",
        "        \n",
        "  return bic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgQv_vKbVFKU"
      },
      "outputs": [],
      "source": [
        "# Training part\n",
        "n_clusters = 4\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "kmeans.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MT_XErMV-wx"
      },
      "outputs": [],
      "source": [
        "# User case\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# load data\n",
        "df = pd.read_csv('活跃用户分析.csv')\n",
        "\n",
        "# substract\n",
        "x = df[['年龄指数','参加活动指数','入会时间指数']].iloc[:,:].values\n",
        "\n",
        "# Choose the best K\n",
        "K = [1,2,3,4,5,6,7,8]\n",
        "GSSE = []\n",
        "for k in K:\n",
        "    SSE = []\n",
        "    kmeans = KMeans(n_clusters = k, random_state = 10)\n",
        "    kmeans.fit(x)\n",
        "    labels = kmeans.labels_\n",
        "    centers = kmeans.cluster_centers_\n",
        "    for label in set(labels):\n",
        "        SSE.append(np.sum(np.sum(x[labels == label,]-centers[label,:]**2)))\n",
        "    GSSE.append(np.sum(SSE))\n",
        "    \n",
        "# Plot the relationship between K and SSE(sum of squared errors)\n",
        "plt.plot(K,GSSE,\"b*-\")\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"SSE\")\n",
        "plt.title(\"Choose the best K\")\n",
        "plt.show()\n",
        "    \n",
        "# Define the model \n",
        "model = KMeans(n_clusters = 5)\n",
        "# Model fit\n",
        "model.fit(x)\n",
        "# Assign a cluster to each instance\n",
        "yhat = model.predict(x)\n",
        "# Create a sample scatter graph\n",
        "# plt.scatter(x[:,0],x[:,1],c = yhat,s= 50,cmap = 'viridis')\n",
        "\n",
        "# plot 3D clustering results\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "ax = plt.subplot(111, projection='3d')  # Create a 3D drawing\n",
        "# Divide the data points into three parts for drawing, with distinction in color\n",
        "ax.scatter(x[:,0],x[:,1],x[:,2],c = yhat,s= 5,cmap = 'viridis')\n",
        "centers = model.cluster_centers_\n",
        "\n",
        "# Create a scatter graph of clustering results\n",
        "ax.scatter(centers[:,0],centers[:,1],centers[:,2],c='red',s=100,alpha=0.5)\n",
        "\n",
        "# Plot\n",
        "pyplot.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzJE_c5idXMX"
      },
      "source": [
        "## Classification tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supported Vector Machine (SVC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with SVM - not effective enough compared to other models\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameter =  {'C': [0.1, 1, 10],\n",
        "              'gamma': [0.1, 1, 10, 100],\n",
        "              'kernel': ['rbf', 'linear']}\n",
        "SVM_model = SVC(kernel = 'linear', C= 1)\n",
        "#SVM_model = GridSearchCV(SVC(), parameter,verbose=2)\n",
        "SVM_model.fit(training_features, y_train)\n",
        "#print(SVM_model.best_params_)\n",
        "\n",
        "y_train_pred = SVM_model.predict(training_features)\n",
        "accuracy = accuracy_score(y_train, y_train_pred)\n",
        "print(f\"Train accuracy: {accuracy}\")\n",
        "\n",
        "# Predict labels for validation set and evaluate performance\n",
        "y_val_pred = SVM_model.predict(val_features)\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f\"Validation accuracy: {accuracy}\")\n",
        "\n",
        "# Predict labels for test set and generate submission file\n",
        "y_test_pred = SVM_model.predict(testing_features)\n",
        "submission = pd.DataFrame({\"ID\": range(test_data.shape[0]), \"Predicted\": y_test_pred})\n",
        "submission.to_csv(\"submission_svm.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrgURB4ZfOUJ"
      },
      "source": [
        "### Multi-layer classifier (neural network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7fOfLNbdl5V"
      },
      "outputs": [],
      "source": [
        "# 50,50 is the pair which gets best result \n",
        "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(50,),max_iter = 50,verbose = True,random_state=1)\n",
        "clf.fit(X, y)\n",
        "# Model score\n",
        "r = clf.score(X, y)\n",
        "print(\"Accuracy:\", r)\n",
        "# Prediction\n",
        "y_predict = clf.predict(test_df.fillna(0))  \n",
        "y_predict = pd.DataFrame(y_predict, columns=['label'])\n",
        "y_predict.to_csv(\"NN.csv\", index=True, index_label='Id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTycmPeOfo_I"
      },
      "outputs": [],
      "source": [
        "# Tuning the model\n",
        "parameter = {\"hidden_layer_sizes\": range(30,100,10),\n",
        "              \"max_iter\": range(30,120,10)}\n",
        "NN_model = MLPClassifier(verbose = True,solver= 'adam')\n",
        "clf = RandomizedSearchCV(NN_model, parameter) # Can change the cv algorithm here, but randomizedsearchcv is faster here\n",
        "clf.fit(X, y)\n",
        "print(clf.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_jcfAjTfsT8"
      },
      "outputs": [],
      "source": [
        "# Model score\n",
        "r = clf.score(X, y)\n",
        "print(\"Accuracy:\", r)\n",
        "# Prediction\n",
        "y_predict = clf.predict(test_df.fillna(0))  \n",
        "y_predict = pd.DataFrame(y_predict, columns=['label'])\n",
        "y_predict.to_csv(\"NN-1.csv\", index=True, index_label='Id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the model results\n",
        "plt.plot(clf.loss_curve_,'r')\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"MCP Loss Evolution\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b4kLwBMf42j"
      },
      "outputs": [],
      "source": [
        "# Print the result of GridSearchCV(if used)\n",
        "pd.DataFrame(clf.cv_results_).sort_values('rank_test_score').head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8adk20AgBzr"
      },
      "outputs": [],
      "source": [
        "# Cross validation\n",
        "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(50,),max_iter= 120,verbose = True,random_state=1)\n",
        "clf.fit(X_train, y_train)\n",
        "# Model score\n",
        "y_pred = clf.predict(X_test.fillna(0)) \n",
        "print(\"f1 score macro is :\",f1_score(y_test, y_pred, average='macro'))\n",
        "print(\"f1 score micro is :\",f1_score(y_test, y_pred, average='micro'))\n",
        "print(\"f1 score weighted is :\",f1_score(y_test, y_pred, average='weighted'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTAsdo-IgSXS"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT-6wbL1gVe1"
      },
      "outputs": [],
      "source": [
        "!pip install xgboost --upgrade\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSDy8vEtgWKT"
      },
      "outputs": [],
      "source": [
        "parameter = {'n_estimators': [50, 100, 150]} \n",
        "             'learning_rate' : [0.05, 0.15, 0.30],\n",
        "             'max_depth' : [5, 10, 20],\n",
        "             'min_child_weight' : [ 1, 3, 7],\n",
        "             'gamma': [0.0, 0.2, 0.4],\n",
        "             'colsample_bytree' : [0.3, 0.5, 0.7]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NjWdU8GkpeI"
      },
      "outputs": [],
      "source": [
        "xgb_model = xgb.XGBClassifier(n_jobs=1)\n",
        "clf = GridSearchCV(xgb_model, parameter)\n",
        "clf.fit(X, y)\n",
        "print(clf.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw2YwYc9ksPK"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(clf.cv_results_).sort_values('rank_test_score').head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gRZ8ZD6ku7A"
      },
      "outputs": [],
      "source": [
        "pred_y = clf.predict(test_df.fillna(0))\n",
        "pred_df = pd.DataFrame(pred_y, columns=['label'])\n",
        "pred_df.to_csv(\"xgb.csv\", index=True, index_label='Id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFZWzwWlk27y"
      },
      "source": [
        "### CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGuoZuf-k6Kt"
      },
      "outputs": [],
      "source": [
        "!pip install catboost\n",
        "from catboost import CatBoostRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfVHq5j_k62x"
      },
      "outputs": [],
      "source": [
        "# Get the index of categorical features\n",
        "cat_fea_idx = np.where(X.dtypes != np.float)[0]\n",
        "cat_model = CatBoostClassifier(iterations=1000 ,\n",
        "                               depth=4,\n",
        "                               od_type=\"Iter\",\n",
        "                               early_stopping_rounds=500,\n",
        "                               loss_function='MultiClass' )\n",
        "cat_model.fit(X, y)\n",
        "pred_y = cat_model.predict(test_df.fillna(0))\n",
        "pred_df = pd.DataFrame(pred_y, columns=['label'])\n",
        "pred_df.to_csv(\"cat.csv\", index=True, index_label='Id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train logistic regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(training_features, y_train)\n",
        "\n",
        "y_train_pred = model.predict(training_features)\n",
        "accuracy = accuracy_score(y_train, y_train_pred)\n",
        "print(f\"Train accuracy: {accuracy}\")\n",
        "\n",
        "# Predict labels for validation set and evaluate performance\n",
        "y_val_pred = model.predict(valling_features)\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f\"Validation accuracy: {accuracy}\")\n",
        "\n",
        "# Predict labels for test set and generate submission file\n",
        "y_test_pred = model.predict(testing_features)\n",
        "submission = pd.DataFrame({\"ID\": range(test_data.shape[0]), \"Predicted\": y_test_pred})\n",
        "submission.to_csv(\"submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for LG(not quite help)\n",
        "param_grid = [\n",
        "    {'penalty' : ['l2','l1','none'],\n",
        "    'C' : np.logspace(-4, 4, 10),\n",
        "    'solver' : ['lbfgs','newton-cg'],\n",
        "    'max_iter' : [100, 1000,2500, 5000]\n",
        "    }\n",
        "]\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV # Faster than Grid search\n",
        "clf = RandomizedSearchCV(model, param_distributions = param_grid, cv = 3, verbose=True, n_jobs=-1)\n",
        "best_clf = clf.fit(training_features, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print (f'Accuracy - : {best_clf.score(training_features,y_train):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Empirical parameters from the internet\n",
        "lgb_params = {\n",
        "    'learning_rate': 0.007,\n",
        "        'max_depth': 5,\n",
        "        'task': 'train',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'binary',\n",
        "         'feature_fraction': 0.8,\n",
        "         'num_leaves':30,\n",
        "        'min_data_in_leaf':100,\n",
        "        'lambda_l1': 0.8,\n",
        "        'lambda_l2':0.9,\n",
        "        'n_estimators':50}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with lightgbm\n",
        "lgb_model = lgb.LGBMClassifier()\n",
        "lgb_model.set_params(**lgb_params)\n",
        "lgb_model.fit(training_features, y_train)\n",
        "\n",
        "y_train_pred = lgb_model.predict(training_features)\n",
        "accuracy = accuracy_score(y_train, y_train_pred)\n",
        "print(f\"Train accuracy: {accuracy}\")\n",
        "\n",
        "# Predict labels for validation set and evaluate performance\n",
        "y_val_pred = lgb_model.predict(valling_features)\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f\"Validation accuracy: {accuracy}\")\n",
        "\n",
        "# Predict labels for test set and generate submission file\n",
        "y_test_pred = lgb_model.predict(testing_features)\n",
        "submission = pd.DataFrame({\"ID\": range(test_data.shape[0]), \"Predicted\": y_test_pred})\n",
        "submission.to_csv(\"lgb_submission.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for LGBM(not very helpful at the end)\n",
        "param_grid = [\n",
        "    {'objetive' : ['binary'],\n",
        "    'learning_rate' : [0.001,0.007,0.0001],\n",
        "    'max_depth' : [1,3,5],\n",
        "    'n_estimators': [50, 100,500, 1000],\n",
        "    'lambda_l1': np.linspace(0,1,11),\n",
        "    'lambda_l2': np.linspace(0,1,11)\n",
        "    }\n",
        "]\n",
        "\n",
        "model = lgb.LGBMClassifier()\n",
        "\n",
        "clf = RandomizedSearchCV(model, param_distributions = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
        "best_clf = clf.fit(training_features, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_clf.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print (f'Accuracy - : {best_clf.score(training_features,y_train):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "feature_imp = pd.DataFrame(sorted(zip(lgb_model.feature_importances_,features)), columns=['Value','Feature']).sort_values(by='Value',ascending=False)\n",
        "\n",
        "# Use the inherited function in lgbm to visualize the top 20 features\n",
        "plt.figure(figsize=(6,6))\n",
        "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[:20])\n",
        "plt.title('LightGBM Features (avg over folds)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Randomforest - overfitting, not used\n",
        "rdf = RandomForestClassifier(n_estimators=50)\n",
        "rdf.fit(training_features, y_train)\n",
        "y_train_pred = rdf.predict(training_features)\n",
        "accuracy = accuracy_score(y_train, y_train_pred)\n",
        "print(f\"Train accuracy: {accuracy}\")\n",
        "\n",
        "# Predict labels for test set and generate submission file\n",
        "y_test_pred = rdf.predict(testing_features)\n",
        "submission = pd.DataFrame({\"ID\": range(test_data.shape[0]), \"Predicted\": y_test_pred})\n",
        "submission.to_csv(\"rf_submission.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8vlxyWZfao8"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For classification tasks, the metrics are usually based on the confusion matrix and yield many derivatives according to the specific user cases.\n",
        "Here are some examples:\n",
        "\n",
        "Accuracy: Accuracy measures the proportion of correctly classified instances out of the total instances. It is suitable when the classes are balanced. However, it may not be the best metric for imbalanced datasets.\n",
        "Use Case: Suitable for tasks like email spam detection or image recognition when the classes are roughly equal in size.\n",
        "\n",
        "Precision: Precision measures the proportion of true positive predictions among all positive predictions. It is used to evaluate how well the model identifies true positive instances.\n",
        "Use Case: Valuable when the cost of false positives is high, such as in medical diagnoses or fraud detection.\n",
        "\n",
        "Recall (Sensitivity or True Positive Rate): Recall measures the proportion of true positive predictions among all actual positive instances. It assesses how well the model captures all positive instances.\n",
        "Use Case: Important in scenarios where missing positive cases is costly, like disease detection or search and rescue operations.\n",
        "\n",
        "F1-Score: The F1-Score is the harmonic mean of precision and recall, balancing both metrics. It is useful when you want to find a trade-off between precision and recall.\n",
        "Use Case: Often used in situations where precision and recall are both important, such as information retrieval or recommendation systems.\n",
        "\n",
        "ROC Curve and AUC: Receiver Operating Characteristic (ROC) curves plot the true positive rate against the false positive rate at various thresholds. The Area Under the Curve (AUC) summarizes the overall performance. It is robust to class imbalance.\n",
        "Use Case: Suitable when you need to compare models' ability to discriminate between classes, e.g., in medical diagnosis or credit scoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For classification task Confusion matrix and several related metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Create heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized, annot=True, fmt=\".2%\", cmap=\"Reds\")\n",
        "\n",
        "# Get class labels from indexes using the idx_to_class dictionary\n",
        "class_labels = [id2label[str(label_idx)] for label_idx in sorted(set(true_labels))]\n",
        "\n",
        "plt.title(\"Confusion Matrix (Accuracy: {:.2%})\".format(accuracy))\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "\n",
        "# Set x-axis and y-axis labels using class labels\n",
        "plt.xticks(np.arange(len(class_labels)) + 0.5, class_labels)\n",
        "plt.yticks(np.arange(len(class_labels))+ 0.5, class_labels)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"Accuracy: {:.2%}\".format(accuracy))\n",
        "print(\"F1 Score: {:.2%}\".format(f1))\n",
        "print(\"Precision: {:.2%}\".format(precision))\n",
        "print(\"Recall: {:.2%}\".format(recall))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.ticker as mticker\n",
        "\n",
        "# Count the occurrences of each class in false_pred\n",
        "class_counts = {}\n",
        "for label in false_pred:\n",
        "    if label in class_counts:\n",
        "        class_counts[label] += 1\n",
        "    else:\n",
        "        class_counts[label] = 1\n",
        "\n",
        "# Calculate the total count\n",
        "total_count = sum(class_counts.values())\n",
        "\n",
        "# Calculate the percentage for each class\n",
        "class_labels = [id2label[str(label)] for label in class_counts.keys()]\n",
        "class_percentages = [(count / total_count) * 100 for count in class_counts.values()]\n",
        "\n",
        "# Create a bar plot of class proportions\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(class_labels, class_percentages)\n",
        "plt.xlabel('Class Labels')\n",
        "plt.ylabel('Percentage')\n",
        "plt.title('Proportion of Each Class in False Predictions')\n",
        "\n",
        "# Add percentage sign to y-axis labels\n",
        "plt.gca().yaxis.set_major_formatter(mticker.PercentFormatter())\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkfcop5MdcMS"
      },
      "outputs": [],
      "source": [
        "# Plotting ROC Curve\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred_prob = logreg.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.title('ROC curve for xx classifier')\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaZVzUr-lmop"
      },
      "source": [
        "## Regression Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In fact, several models shown above used for classification task have their regressor too, I don't repeat here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t1H5Xx2lqPC"
      },
      "source": [
        "### LightGBM(Model Interpretation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkEt6KqimC0P"
      },
      "outputs": [],
      "source": [
        "# Model Interpretaion\n",
        "! pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka9MMQW9miIx"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "shap.initjs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChI5W1k7mvmf"
      },
      "outputs": [],
      "source": [
        "# Instantiation of a LGBMRegressor model\n",
        "\n",
        "lgbm_reg = LGBMRegressor(n_estimators = 200, learning_rate = 0.07, n_jobs = -1, random_state = 42)\n",
        "lgbm_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict log_SalePrice for the variables in the training set\n",
        "y_pred_lgbm_train = lgbm_reg.predict(X_train_scaled)\n",
        "\n",
        "rmse_lgbm_train = np.sqrt(metrics.mean_squared_error(y_train, y_pred_lgbm_train))\n",
        "r2_lgbm_train = metrics.r2_score(y_train, y_pred_lgbm_train)\n",
        "print(\"Training RMSE:\", rmse_lgbm_train)\n",
        "print(\"Training R²:\", r2_lgbm_train)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Predict log_SalePrice for the variables in the validation set\n",
        "y_pred_lgbm = lgbm_reg.predict(X_val_scaled)\n",
        "\n",
        "rmse_lgbm = np.sqrt(metrics.mean_squared_error(y_val, y_pred_lgbm))\n",
        "r2_lgbm = metrics.r2_score(y_val, y_pred_lgbm)\n",
        "print(\"Out-of-sample RMSE:\", rmse_lgbm)\n",
        "print(\"Out-of-sample R²:\", r2_lgbm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6sqeMQ_CLc8"
      },
      "outputs": [],
      "source": [
        "# Print feature importance\n",
        "lgb.plot_importance(lgbm_reg)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7uiNTtnm5Yv"
      },
      "outputs": [],
      "source": [
        "# Save the model result for later use\n",
        "dump(lgbm_reg, 'lgbm_reg.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-UPgJo_m-hW"
      },
      "outputs": [],
      "source": [
        "# Load the pretrained model\n",
        "loaded_model = load('lgbm_reg.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlHTR20cnIAp"
      },
      "outputs": [],
      "source": [
        "# Plot one of the decision tree to see how the criteria work\n",
        "lightgbm.plot_tree(lgbm_reg,tree_index=0,figsize=(100,50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOyBaJ_SnMph"
      },
      "outputs": [],
      "source": [
        "custom_labels = ['', '', '']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7OQXmW0narr"
      },
      "outputs": [],
      "source": [
        "# Try to use SHAP value to interprete the model\n",
        "explainer = shap.Explainer(lgbm_reg)\n",
        "shap_values = explainer(X_train_scaled)\n",
        "\n",
        "shap.summary_plot(shap_values, X_train_scaled, plot_type = \"bar\",feature_names=custom_labels)\n",
        "shap.plots.bar(shap_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfH8XJNznbXJ"
      },
      "outputs": [],
      "source": [
        "# The impact of the feature on model output\n",
        "shap.summary_plot(shap_values, X_train_scaled,feature_names=custom_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIsXDmBwndrX"
      },
      "outputs": [],
      "source": [
        "# Plot the heatmap for each instance\n",
        "shap_values_explaination = shap.Explanation(shap_values, feature_names=custom_labels) \n",
        "shap.plots.heatmap(shap_values_explaination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBElrwq8nlNJ"
      },
      "outputs": [],
      "source": [
        "# Change the column name\n",
        "shap_values.feature_names = custom_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTWRZdd4nnVJ"
      },
      "outputs": [],
      "source": [
        "# Reverse the scaler\n",
        "shap_values.data = scaler.inverse_transform(shap_values.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za4tRVcZnqti"
      },
      "outputs": [],
      "source": [
        "# Plot only one instance\n",
        "shap.plots.waterfall(shap_values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mean Absolute Error (MAE): MAE measures the average absolute difference between predicted and actual values. It gives equal weight to all errors.\n",
        "Use Case: Suitable for tasks like predicting house prices, where you want to quantify the average magnitude of errors.\n",
        "\n",
        "Mean Squared Error (MSE): MSE calculates the average of the squared differences between predicted and actual values. It penalizes larger errors more heavily.\n",
        "Use Case: Commonly used for regression problems like stock price prediction or weather forecasting.\n",
        "\n",
        "Root Mean Squared Error (RMSE): RMSE is the square root of MSE, providing an interpretable metric in the same units as the target variable.\n",
        "Use Case: Useful when you want to express the model's error in a more interpretable way, such as predicting product sales.\n",
        "\n",
        "R-squared (Coefficient of Determination): R-squared measures the proportion of the variance in the dependent variable that is explained by the independent variables. It ranges from 0 to 1, with higher values indicating better model fit.\n",
        "Use Case: Often used to assess the goodness-of-fit of regression models, e.g., in economic forecasting or scientific research."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(actual, predicted)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "\n",
        "# Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(actual, predicted)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# Root Mean Squared Error (RMSE)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Root Mean Squared Error:\", rmse)\n",
        "\n",
        "# R-squared (R2)\n",
        "r2 = r2_score(actual, predicted)\n",
        "print(\"R-squared (R2):\", r2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
