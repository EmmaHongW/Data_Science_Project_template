{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here is the template for data science project, this notebook focus exclusively on exploratory data analysis part:\n",
        "\n",
        "By Emma HONG, DSBA master student from CentraleSuperlec"
      ],
      "metadata": {
        "id": "Ab5Rgjr109uK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ordinary EDA"
      ],
      "metadata": {
        "id": "gpZxbmpe2HLH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "p8O6231604DT"
      },
      "outputs": [],
      "source": [
        "# import the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# for modeling\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as stats\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "#from skopt import BayesSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "import joblib\n",
        "\n",
        "# for scoring\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error # regression\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, f1_score, accuracy_score, hinge_loss\n",
        "\n",
        "# for visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.pylab import rcParams\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from statsmodels.graphics.tsaplots import plot_pacf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google Drive for later use\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hAZCGCQo1zKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the files in the root filefolder\n",
        "import os\n",
        "for dirname, _, filenames in os.walk(os.getcwd()):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "CY4amQYNHerQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic"
      ],
      "metadata": {
        "id": "aumjHb-pbVA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from files\n",
        "path = ''\n",
        "headnum = 1 # Change it according to the real case\n",
        "separater = ',' # Sometimes it's ';'\n",
        "# CSV file\n",
        "df = pd.read_csv(path,sep=separater) # add this code when you don't want certain columns: usecols = lambda x: x not in ['', ''] \n",
        "\n",
        "# Json file\n",
        "df = pd.read_json(path)\n",
        "# If you want to turn json file into a \n",
        "key = '' # Column name for the key\n",
        "value = '' # Column name for the value\n",
        "dictionary = df.set_index(key)[value].to_dict()\n",
        "\n",
        "# Excel file\n",
        "df = pd.read_excel(path,header = headnum)"
      ],
      "metadata": {
        "id": "KmApBFHt12OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifically, if you want to merge the dataframe based on same columns\n",
        "df = pd.concat([df1,df2], axis = 0) # add this when you only want to merge same columns: join='inner'\n",
        "# or use methods similar to sql\n",
        "df_merge = df.merge(df1, how ='left', right_on='right_id', left_on='left_id')"
      ],
      "metadata": {
        "id": "tX1FsWDlJCK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy if the later processing will overwrite the dataframe\n",
        "df_copy = df.copy()"
      ],
      "metadata": {
        "id": "TJN8FXvcJ8-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is the codes to download data from Kaggle\n",
        "! pip install kaggle\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json # Get your kaggle.json from Kaggle website in advance\n",
        "\n",
        "# Import a toy kaggle dataset to try the model interaction\n",
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "kagglepath = '' # The link of the target file page\n",
        "od.download(kagglepath)"
      ],
      "metadata": {
        "id": "xORKfnE13R_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the first look on the data\n",
        "df.head() # You can specify the rows to view here"
      ],
      "metadata": {
        "id": "Nt4uGa6o4kFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To be more clearer, show the meta information of each column\n",
        "df.info()"
      ],
      "metadata": {
        "id": "QZSh2YnB43do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate descriptive statistics\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "X2QOUSRAGD0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to adjust the format of some columns manually\n",
        "# Encode the time format column\n",
        "columnname = ''\n",
        "typename = % # object, int64, float64, bool, datetime64, timedelta, category\n",
        "df[columnname] = pd.to_datetime(df[columnname])\n",
        "# Change the type for a specific column\n",
        "df[columnname] = df[columnname].astype(typename)"
      ],
      "metadata": {
        "id": "nZDSQDBZ5GOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of rows for each column\n",
        "df.count()"
      ],
      "metadata": {
        "id": "gRT9Z1LUE-sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of rows contain na value\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "qJH2U7xx4iPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the number of missing value and frequency\n",
        "\n",
        "missing_values = df.isna().sum()\n",
        "\n",
        "df_missing = pd.DataFrame(data = missing_values, columns = ['count'], index = dict(missing_values).keys())\n",
        "df_missing['frequency'] = missing_values / df.shape[0]\n",
        "df_missing = df_missing.sort_values(by = 'count', ascending = False)\n",
        "\n",
        "df_missing.head(20)"
      ],
      "metadata": {
        "id": "N3yW9pOJQRLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the dataframe has duplicated rows(can limit to specific columns if you want)\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "MmxdlyCuKTP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distribution"
      ],
      "metadata": {
        "id": "aHK2b1zYYnpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of the values in certain column (categorical columns)\n",
        "cat_features = ['','','']\n",
        "\n",
        "# Create a figure with subplots\n",
        "# May need to change the nrows and ncols to suit the specific case\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(cat_features), figsize=(len(cat_features)*6, 6))\n",
        "\n",
        "# Loop through each categorical column and plot its value counts on a subplot\n",
        "for i, columnname in enumerate(cat_features):\n",
        "    df[columnname].value_counts().plot(kind='bar', ax=axes[i]) #kind = 'barh' if you prefer horizontal one\n",
        "    axes[i].set_title(columnname)\n",
        "\n",
        "# Adjust spacing between subplots and show the figure\n",
        "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oxcjrTeEFKck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another way to plot the distribution of caterical data(The input df should only contain categorical columns)\n",
        "# Distribution graphs (histogram/bar graph) of column data\n",
        "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
        "    nunique = df.nunique()\n",
        "    nRow, nCol = df.shape\n",
        "    columnNames = list(df)\n",
        "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
        "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k') # white/black\n",
        "    for i in range(min(nCol, nGraphShown)):\n",
        "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
        "        columnDf = df.iloc[:, i]\n",
        "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
        "            # To increase the readibility, only show top 50 results here\n",
        "            valueCounts = columnDf.value_counts().head(50)\n",
        "            valueCounts.plot.bar()\n",
        "        else:\n",
        "            columnDf.hist()\n",
        "        plt.ylabel('counts')\n",
        "        plt.xticks(rotation = 90)\n",
        "        plt.title(f'{columnNames[i]} (column {i})')\n",
        "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "75JOiRbQugzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of the values in certain column (numerical columns)\n",
        "num_features = ['','','']\n",
        "df[list_num_features].hist(bins = 100, figsize = (30, 25));"
      ],
      "metadata": {
        "id": "a2WYP1emb55V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another way to show the variable distribution\n",
        "optimal_bins_target = round(math.log(df.shape[0], 2) + 1) # based on Sturge's Rule: Optimal Bins = ⌈log_2(n) + 1⌉\n",
        "df[columnname].hist(bins = optimal_bins_target)\n",
        "\n",
        "# Print the skewness and kurtosis value\n",
        "print(\"Skewness: %.2f\" % df['valeurfonc'].skew())\n",
        "print(\"Kurtosis: %.2f\" % df['valeurfonc'].kurt())"
      ],
      "metadata": {
        "id": "CcRWFGdxRd5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check distribution of numerical value\n",
        "fig, axes = plt.subplots(1,3, figsize=(21,6))\n",
        "sns.distplot(df[columnname], ax=axes[0])\n",
        "sns.distplot(np.log1p(df[columnname]), ax=axes[1])\n",
        "axes[1].set_xlabel('log(1+price)')\n",
        "# QQ plot\n",
        "sm.qqplot(np.log1p(df[columnname]), stats.norm, fit=True, line='45', ax=axes[2])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jmskz2BhYTAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculation about numerical column\n",
        "pd.set_option('display.float_format',lambda x : '%.2f' % x) # Cancel scientific notation\n",
        "\n",
        "groupcolumn = ''\n",
        "result = df.groupby(groupcolumn).agg({columnname: ['mean', 'min', 'max']})\n",
        "print(\"Mean, min, and max values of price_per_square grouped by groupcolumn\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "s1m6IfMQZG9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation"
      ],
      "metadata": {
        "id": "_dLtdX6kYeXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Relationship between numerical data\n",
        "numericallist = ['','','']\n",
        "dfnum = df[numericallist]\n",
        "# Only Sample 500 to get the intuition, use these codes when the dataset has large volume\n",
        "df_numsome = df_num.sample(n=500)\n",
        "sns.pairplot(df_numsome)"
      ],
      "metadata": {
        "id": "nofFQTiDGAwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# When the data distribution has extreme outliers\n",
        "for col in numericallist:\n",
        "    df_numsome[col] = np.log(df_numsome[col])\n",
        "sns.pairplot(df_numsome)"
      ],
      "metadata": {
        "id": "6VM1uYjoG2Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions to remove the outliers\n",
        "def remove_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] > lower_bound) & (df[column] < upper_bound)]\n",
        "df_ro = remove_outliers(df,columnname)"
      ],
      "metadata": {
        "id": "9gbUeeV6HUfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the distibution after removing the outliers\n",
        "\n",
        "def without_outlier(df, columnname):\n",
        "    # Calculate interquartile range of the prices\n",
        "    q25 = np.percentile(df[columnname].values, 25)\n",
        "    q75 = np.percentile(df[columnname].values, 75)\n",
        "    IQR = q75 - q25\n",
        "\n",
        "    # Calculate the outlier cutoff\n",
        "    lower_bound = q25 - IQR*1.5\n",
        "    upper_bound = q75 + IQR*1.5\n",
        "\n",
        "    # Identify outliers and remove them for the data visualization\n",
        "    fig = plt.figure(figsize = (11, 7))\n",
        "    ax = fig.gca()\n",
        "    valueinside = df.loc[(df[columnname] >= lower_bound) & (df[columnname] <= upper_bound)][columnname]\n",
        "    valueinside.hist(bins = 100, ax = ax)\n",
        "    plt.show()\n",
        "\n",
        "    # Display the Skewness and Kurtosis of the prices without the outliers\n",
        "    print(\"Skewness: %.2f\" % valueinside.skew())\n",
        "    print(\"Kurtosis: %.2f\" % valueinside.kurt())"
      ],
      "metadata": {
        "id": "J-TVRUDGIQsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation heatmap(prettier, show only half of the graph)\n",
        "# Change the style theme\n",
        "sns.set(style = \"white\")\n",
        "\n",
        "non_numeric_columns = ['','','']\n",
        "# Compute the correlation matrix\n",
        "corr = df.drop(non_numeric_columns, axis = 1).corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype = bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "fig, axes = plt.subplots(figsize = (11, 9))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap = True)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask = mask, cmap = cmap, center = 0,\n",
        "            square = True, linewidths = .5, cbar_kws = {\"shrink\": .5}); # cbar_kws: colorbar setting"
      ],
      "metadata": {
        "id": "9oFv1BehbZFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the top 15 correlated features to the dependent variable\n",
        "targetcolumn = ''\n",
        "correlated_features = corr[targetcolumn].sort_values(ascending = False)\n",
        "\n",
        "correlated_features[1:16]"
      ],
      "metadata": {
        "id": "7VYD110QPjY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the top 15 in a new DataFrame\n",
        "\n",
        "top_15_correlated_features = correlated_features[1:16].index\n",
        "\n",
        "df_top15_correlated_features = df[top_15_correlated_features]\n",
        "\n",
        "df_top15_correlated_features.head(10)"
      ],
      "metadata": {
        "id": "xOiDwLrmP7Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the correlation between the top 15 features(They may be highly correlated between each other)\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr_top15 = df[top_15_correlated_features].corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask_top15 = np.triu(np.ones_like(corr_top15, dtype = bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "fig, axes = plt.subplots(figsize = (11, 9))\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr_top15, mask = mask_top15, cmap = cmap, center = 0,\n",
        "            square = True, linewidths = .5, cbar_kws = {\"shrink\": .5});"
      ],
      "metadata": {
        "id": "Rs-q5_h2RE1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing with time series data"
      ],
      "metadata": {
        "id": "Edzd-tflTt5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datecolumn = ''\n",
        "valuecolumn = ''\n",
        "# The distribution of data by time ( For time series data)\n",
        "datecolumn = ''\n",
        "df[datecolumn] = pd.to_datetime(df[datecolumn], format='%m/%d/%Y') # The format of date can change\n",
        "df[datecolumn] = df[datecolumn].dt.strftime('%Y-%m-%d')\n",
        "df[datecolumn].value_counts().sort_index().plot(kind = 'line')"
      ],
      "metadata": {
        "id": "xc02zmM0Txlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For panel data\n",
        "# Creating another dataset so we can re-use the original if we need it\n",
        "df_ = df.copy()\n",
        "df_ = df.sort_values(by=[datecolumn])\n"
      ],
      "metadata": {
        "id": "Hs6Yb4OyZvBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resample the data by month\n",
        "dfM = df_.resample('M').mean()\n",
        "# check the boxplot of average price by month\n",
        "dfM.loc[:,[valuecolumn]].boxplot()"
      ],
      "metadata": {
        "id": "iJNMujg1Z5Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the price change with the time\n",
        "fig, ax = plt.subplots(figsize=(12,6))\n",
        "dfM[valuecolumn].plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BDWvRcVAZ8VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter other factor\n",
        "factor = ''\n",
        "xx = 10 # customized value\n",
        "df_xx = df[df[factor]==xx]\n",
        "df_xx_M = df_xx.resample('M').mean().dropna()\n",
        "df_xx_Y = df_xx.resample('Y').mean()\n",
        "fig, ax = plt.subplots(figsize=(12,6))\n",
        "df_xx_M[valuecolumn].plot()\n",
        "plt.title(\"average value change by month\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AGyg_pVvaJlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(12,6))\n",
        "df_xx_Y[valuecolumn].plot()\n",
        "plt.title(\"average value change by year\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "npLHOry8agqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check seasonality\n",
        "y_m_avg = df_xx_M.groupby(datecolumn)[valuecolumn].mean()\n",
        "y_m_avg = y_m_avg.dropna()\n",
        "\n",
        "decomposition = sm.tsa.seasonal_decompose(y_m_avg, model='additive', period=6)\n",
        "\n",
        "#Gather the trend, seasonality, and residuals\n",
        "trend = decomposition.trend\n",
        "seasonal = decomposition.seasonal\n",
        "residual = decomposition.resid\n",
        "\n",
        "# Plot gathered statistics\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.subplot(411)\n",
        "plt.plot(y_m_avg, label='Original', color='blue')\n",
        "plt.legend(loc='best')\n",
        "plt.subplot(412)\n",
        "plt.plot(trend, label='Trend', color='blue')\n",
        "plt.legend(loc='best')\n",
        "plt.subplot(413)\n",
        "plt.plot(seasonal,label='Seasonality', color='blue')\n",
        "plt.legend(loc='best')\n",
        "plt.subplot(414)\n",
        "plt.plot(residual, label='Residuals', color='blue')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "Lu16Ygpfav-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stationary check\n",
        "\n",
        "def stationarity_check(TS):\n",
        "    \n",
        "    # Import adfuller\n",
        "    from statsmodels.tsa.stattools import adfuller\n",
        "    \n",
        "    # Calculate rolling statistics\n",
        "    roll_mean = TS.rolling(window=8, center=False).mean()\n",
        "    roll_std = TS.rolling(window=8, center=False).std()\n",
        "    \n",
        "    # Perform the Dickey Fuller test\n",
        "    dftest = adfuller(TS) \n",
        "    \n",
        "    # Plot rolling statistics:\n",
        "    fig = plt.figure(figsize=(12,6))\n",
        "    orig = plt.plot(TS, color='blue',label='Original')\n",
        "    mean = plt.plot(roll_mean, color='red', label='Rolling Mean')\n",
        "    std = plt.plot(roll_std, color='black', label = 'Rolling Std')\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Rolling Mean & Standard Deviation')\n",
        "    plt.show(block=False)\n",
        "    \n",
        "    # Print Dickey-Fuller test results\n",
        "    print('Results of Dickey-Fuller Test: \\n')\n",
        "\n",
        "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', \n",
        "                                             '#Lags Used', 'Number of Observations Used'])\n",
        "    for key, value in dftest[4].items():\n",
        "        dfoutput['Critical Value (%s)'%key] = value\n",
        "    print(dfoutput)\n",
        "    \n",
        "    return None"
      ],
      "metadata": {
        "id": "D77LVnjya1p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts_log_decompose = residual\n",
        "ts_log_decompose.dropna(inplace=True)\n",
        "stationarity_check(ts_log_decompose)"
      ],
      "metadata": {
        "id": "Is_D3bvcg94e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check autocorrelation\n",
        "# plot the ACF and PACF\n",
        "rcParams['figure.figsize']=7,5\n",
        "plot_acf(y_m_avg); plt.xlim(0,24); plt.show()\n",
        "plot_pacf(y_m_avg); plt.xlim(0,24); plt.ylim(-1,1);plt.show()"
      ],
      "metadata": {
        "id": "51TNkOxnhDPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using PySpark to deal with big data"
      ],
      "metadata": {
        "id": "IpUPvhEV2LkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration codes\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop2.tgz\n",
        "!tar zxvf spark-3.3.1-bin-hadoop2.tgz\n",
        "!pip install -q findspark\n",
        " \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setMaster(\"local\")\n",
        "sc = SparkContext(conf = conf)\n",
        "print(\"initialization successful!\")\n",
        "\n",
        "\n",
        "#import of the SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#inizialization of the Spark Session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Template\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "\n",
        "seed_value=0\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n"
      ],
      "metadata": {
        "id": "yVCyv9aVxJou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = ''\n",
        "# Load the file into an RDD\n",
        "RDD = sc.textFile(filename)\n",
        "\n",
        "# Create RDD with files, use wholeTextFiles to distinguish the files\n",
        "def load_text(filename):\n",
        "    return sc.wholeTextFiles(filename)\n",
        "# Read the texts into RDD, you can read several files in a time\n",
        "texts = load_text(\"filefolder/*.txt\")"
      ],
      "metadata": {
        "id": "czOzOPpwyaHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data example: \n",
        "\n",
        "[('file:/content/the_office/33793.txt',\n",
        "  \"06x12 - Scott's Tots\\nMikela\\nMikela: We just want to say thanks.\\n\"),\n",
        " ('file:/content/the_office/4199.txt',\n",
        "  '02x08 - Performance Review\\nMichael\\nMichael: What do you say, Jan?\\n'),\n",
        " ('file:/content/the_office/11602.txt',\n",
        "  \"03x08 - The Merger\\nPam\\nPam: Oh, I'll just time him later.\\n\"),\n",
        " ('file:/content/the_office/6578.txt',\n",
        "  '02x18 - Take Your Daughter to Work Day\\nToby\\nToby: [to Sasha] Okay, tell them what you wanted to say.\\n'),\n",
        " ('file:/content/the_office/9020.txt',\n",
        "  '02x99 - Deleted Scenes from Season 2\\nMichael\\nMichael: I used to be in HR. I was a Hell raiser.\\n')]"
      ],
      "metadata": {
        "id": "ajbKYsv7FDaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions frequently used to deal with texts data\n",
        "\n",
        "import string\n",
        "import re\n",
        "from stop_words import get_stop_words\n",
        "\n",
        "# remove punctuations, only remove the punctuations on both sides\n",
        "def remove_punctuation(word):\n",
        "    punctuation = string.punctuation\n",
        "    word = word.strip(punctuation)\n",
        "    return word\n",
        "\n",
        "# remove punctuations\n",
        "def remove_non_letters(word):\n",
        "    if type(word) != str:\n",
        "        word = \"\"\n",
        "    regex = re.compile(r'[^\\w\\s]')\n",
        "    return regex.sub('', word)\n",
        "\n",
        "# remove the stop words\n",
        "def removestopwords(word):\n",
        "    stop_words = get_stop_words('english')\n",
        "    if word in stop_words:\n",
        "      filtered_word = \"\"\n",
        "    else:\n",
        "      filtered_word = word\n",
        "    return filtered_word"
      ],
      "metadata": {
        "id": "coqHM0-IzCOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample codes for conducting MapReduce method on RDDs\n",
        "# Use mapreduce to count the number of words in the whole set of dialogs\n",
        "# Map method can apply the same function to the RDD element wise\n",
        "diagnum = diag.map(lambda x: (x,1))\\\n",
        "              .reduceByKey(lambda x,y: x+y)\\\n",
        "              .sortBy(lambda f: f[1], ascending=False)"
      ],
      "metadata": {
        "id": "aDyZ1GBLzwXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you only want to apply the function on the value of a pair (x,y) within the RDD, use mapValues function\n",
        "# Use case: Find common words by character\n",
        "# Gather the dialogs under each character\n",
        "cha_diag = texts.map(lambda x: (x[1].split(\"\\n\")[1],dialog(x[1].split(\"\\n\")[2]).split(\" \")))\\\n",
        "                .reduceByKey(lambda x,y:x+y)\n",
        "# Use mapValues to only process the value to split the words and clean the words\n",
        "# The flatMap sentence is to spread the (character,word) and then make it easier to count the word frequency for each character\n",
        "rdd = cha_diag.flatMap(lambda x: [(x[0], y) for y in x[1]])\\\n",
        "         .mapValues(lambda word: remove_punctuation(word))\\\n",
        "         .mapValues(lambda word: word.lower())\\ # lower() method is useful to remove the influence of the capitalizartion of the words\n",
        "         .mapValues(lambda x: removestopwords(x))\\\n",
        "         .filter(lambda word: len(word[1]) > 0) # empty string is not useful\n",
        "# Do the mapreduce to get the result\n",
        "# sorted sentence is to sort the tuples by the word frequency\n",
        "rdd1 = rdd.map(lambda x: (x, 1))\\\n",
        "          .reduceByKey(lambda x, y: x+y)\\\n",
        "          .map(lambda x: (x[0][0],(x[0][1],x[1]))).groupByKey()\\\n",
        "          .map(lambda x: (x[0], sorted(x[1], key=lambda y: y[1], reverse=True))) \\\n",
        "          .map(lambda x: (x[0],x[1][:10]))\n",
        "\n",
        "# Here we only put 5 as examples\n",
        "print(\"The 10 most common words for each character is:\",rdd1.take(5))"
      ],
      "metadata": {
        "id": "fnniDxFn0PfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark Dataframe, similar to pandas dataframe\n",
        "# Read the csv series\n",
        "dfs =spark.read.option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").csv(\"XXX.csv\", header=True)\n",
        "dfs.show(5)"
      ],
      "metadata": {
        "id": "JxZwKr5N1oU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Example of data\n",
        "+---+------+-------------+--------------------+-------+-----+----------+--------+--------------+----------+---------------+--------------------+\n",
        "|_c0|Season| EpisodeTitle|               About|Ratings|Votes|Viewership|Duration|          Date|GuestStars|       Director|             Writers|\n",
        "+---+------+-------------+--------------------+-------+-----+----------+--------+--------------+----------+---------------+--------------------+\n",
        "|  0|   1.0|        Pilot|The premiere epis...|    7.5| 4936|      11.2|      23| 24 March 2005|      null|     Ken Kwapis|Ricky Gervais |St...|\n",
        "|  1|   1.0|Diversity Day|Michael's off col...|    8.3| 4801|         6|      23| 29 March 2005|      null|     Ken Kwapis|         B. J. Novak|\n",
        "|  2|   1.0|  Health Care|Michael leaves Dw...|    7.8| 4024|       5.8|      22|  5 April 2005|      null|Ken Whittingham|    Paul Lieberstein|\n",
        "|  3|   1.0| The Alliance|Just for a laugh,...|    8.1| 3915|       5.4|      23| 12 April 2005|      null|   Bryan Gordon|       Michael Schur|\n",
        "|  4|   1.0|   Basketball|Michael and his s...|    8.4| 4294|         5|      23| 19 April 2005|      null|   Greg Daniels|        Greg Daniels|\n",
        "+---+------+-------------+--------------------+-------+-----+----------+--------+--------------+----------+---------------+--------------------+\n",
        "only showing top 5 rows\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "VGSaEWtx60Cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show 5 value as example\n",
        "dfs_distinct = dfs.select(columnname).distinct()\n",
        "dfs_distinct.show(5)"
      ],
      "metadata": {
        "id": "hdKrV_z43ZBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the number of episodes each director directed\n",
        "# Using parquet to read dataframe and use pandas function to count the number of episodes each director directed\n",
        "# Finally transform pandas dataframe into spark dataframe\n",
        "dfs = dfs.select(\"Director\",\"EpisodeTitle\")\n",
        "dfs.write.parquet(\"template/sample\",mode='overwrite')\n",
        "df = pd.read_parquet(\"template/sample\")\n",
        "# Use pandas dataframe to count\n",
        "pdf = df[\"Director\"].value_counts().sort_values(ascending=False)\n",
        "pdf1 = [(i,v) for i,v in pdf.iteritems()]\n",
        "\n",
        "# Finally tranform the pandas dataframe back to spark dataframe and print out the result\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "schema = StructType([ \\\n",
        "    StructField(\"Director\",StringType(),True), \\\n",
        "    StructField(\"number_of_episode\",IntegerType(),True),\n",
        "  ])\n",
        "rdd = sc.parallelize(pdf1)\n",
        "sdf = spark.createDataFrame(rdd,schema)\n",
        "print(\"The number of episodes each director directed: \")\n",
        "print(sdf.where(\"Director <> ''\").show(5))"
      ],
      "metadata": {
        "id": "7umL6BCp3lLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the 10 most common words for each character\n",
        "import collections\n",
        "# Read the data in pandas dataframe\n",
        "df = pd.read_csv(\"XXX.csv\",sep=';')\n",
        "\n",
        "# Deal with the words\n",
        "df[\"words\"] = df[\"Field\"].apply(lambda x: dialog(x)).apply(lambda x: x.split(\" \"))\n",
        "df1 = df.explode(\"words\") # Spread the words in rows\n",
        "df1[\"words\"] = df1[\"words\"].apply(remove_punctuation) # Remove the punctuation in both sides\n",
        "df1[\"words\"] = df1[\"words\"].apply(lambda x: x.lower())# Convert the words to lowercase\n",
        "df1[\"words\"] = df1[\"words\"].apply(removestopwords)# Remove stopwords\n",
        "df1 = df1[df1[\"words\"] != \"\"] # Filter the empty elements\n",
        "# Count the word frequency by characters and save the top 10 common words\n",
        "top10_words = df1.groupby(\"Field1\")\n",
        "top10_words = top10_words.apply(lambda x: collections.Counter(x[\"words\"]).most_common(10))\n",
        "top10_words = pd.DataFrame(top10_words)\n",
        "\n",
        "print(\"The 10 most common words for each character are:\")\n",
        "# Here we take 5 characters as examples:\n",
        "top10_words.head(5)"
      ],
      "metadata": {
        "id": "PwvkWPAe4WEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the 10 episodes having the most number of words\n",
        "from pyspark.sql.functions import explode, count,split,desc,lower,udf\n",
        "# Select the dialogs and episodes to process\n",
        "dfs = dfst.select(\"Field\",\"Text\")\n",
        "dfs = dfs.withColumnRenamed(\"Text\",\"episode\")\n",
        "# Count the words for each episode\n",
        "udf_dialog = udf(dialog)\n",
        "dfs = dfs.withColumn(\"dialogs\",udf_dialog(col(\"Field\"))) # extract the dialog\n",
        "dfs = dfs.withColumn(\"words\", explode(split(\"dialogs\", \" \"))) # spread the words\n",
        "word_counts = dfs.groupBy(\"episode\").agg(count(\"words\").alias(\"word_count\")).orderBy(desc(\"word_count\")) # Count the words by episode\n",
        "\n",
        "print(\"The 10 episodes having the most number of words are:\")\n",
        "# Print the result:\n",
        "word_counts.show(10)"
      ],
      "metadata": {
        "id": "-E6adJE74IdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training(including tuning part)"
      ],
      "metadata": {
        "id": "7iwxuW3mHPa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature selection"
      ],
      "metadata": {
        "id": "cgLjYmNxoNai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example from https://www.datacamp.com/tutorial/feature-selection-python\n",
        "# load data\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = pd.read_csv(url, names=names)\n",
        "\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]"
      ],
      "metadata": {
        "id": "-yxJZiyeokIu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries first\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2"
      ],
      "metadata": {
        "id": "HhRpy51SoMw7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filter Method**"
      ],
      "metadata": {
        "id": "iLxl1gF5rBQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Feature extraction\n",
        "test = SelectKBest(score_func=chi2, k=4)\n",
        "fit = test.fit(X, Y)\n",
        "\n",
        "# Summarize scores\n",
        "np.set_printoptions(precision=3)\n",
        "print(dataframe.columns)\n",
        "print(fit.scores_)\n",
        "\n",
        "features = fit.transform(X)\n",
        "# Summarize selected features\n",
        "print(features[0:5,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5JlUtJMoWBm",
        "outputId": "bf24fa94-0c94-4365-c7f0-e107b06e001a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'], dtype='object')\n",
            "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
            "[[148.    0.   33.6  50. ]\n",
            " [ 85.    0.   26.6  31. ]\n",
            " [183.    0.   23.3  32. ]\n",
            " [ 89.   94.   28.1  21. ]\n",
            " [137.  168.   43.1  33. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wrapper Method**"
      ],
      "metadata": {
        "id": "7LqX-W2gq7GM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your necessary dependencies\n",
        "# Recursive Feature Elimination \n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ],
      "metadata": {
        "id": "18yXagbypW4r"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction\n",
        "model = LogisticRegression()\n",
        "rfe = RFE(estimator=model, n_features_to_select=3)\n",
        "fit = rfe.fit(X, Y)\n",
        "print(\"Num Features: %s\" % (fit.n_features_))\n",
        "print(\"Selected Features: %s\" % (fit.support_))\n",
        "print(\"Feature Ranking: %s\" % (fit.ranking_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8D7uxhTpllP",
        "outputId": "30148998-c725-4a1b-e866-9e209e1b6168"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Features: 3\n",
            "Selected Features: [ True False False False False  True  True False]\n",
            "Feature Ranking: [1 2 4 5 6 1 1 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedded Methods**"
      ],
      "metadata": {
        "id": "-DFQX0S6rd0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First things first\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X,Y)\n",
        "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
        "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
        "# A helper method for pretty-printing the coefficients\n",
        "def pretty_print_coefs(coefs, names = None, sort = False):\n",
        "    if names == None:\n",
        "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
        "    lst = zip(coefs, names)\n",
        "    if sort:\n",
        "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
        "    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
        "                                   for coef, name in lst)\n",
        "    \n",
        "print (\"Ridge model:\", pretty_print_coefs(ridge.coef_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In5IQxK1rfJO",
        "outputId": "13dbfd4c-ae59-4068-d922-e5d51aec5c87"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge model: 0.021 * X0 + 0.006 * X1 + -0.002 * X2 + 0.0 * X3 + -0.0 * X4 + 0.013 * X5 + 0.145 * X6 + 0.003 * X7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised learning"
      ],
      "metadata": {
        "id": "6qzszP9bdfdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-means"
      ],
      "metadata": {
        "id": "A8gZY2V0Tfrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unsupervised learning\n",
        "# Elbow method to determine the number k of clusters\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Initialisation of the list of values k\n",
        "number_clusters = range(1, 7)\n",
        "\n",
        "# Initialisation of an empty list 'distortions' which will contain the distortion values of each Kmeans models depending of the number k\n",
        "distortions = []\n",
        "\n",
        "# For each value of k, fit a different KMeans algorithm\n",
        "for k in number_clusters:\n",
        "    kmeans = KMeans(n_clusters = k, random_state = 42)\n",
        "    kmeans.fit(X_scaled)\n",
        "\n",
        "    # Compute the distortion of each constructed model\n",
        "    distortions.append(sum(np.min(cdist(X_scaled, kmeans.cluster_centers_, 'euclidean'), axis = 1)) / np.size(X_scaled, axis = 0))\n",
        "    \n",
        "# Create the distortions graph depending on the number of clusters\n",
        "plt.figure(figsize = (16, 9))\n",
        "plt.plot(number_clusters, distortions, 'gx-')\n",
        "plt.xlabel('Number of Clusters k')\n",
        "plt.ylabel('Distortion (WSS/TSS)')\n",
        "plt.title('Elbow method displaying the optimal number of clusters')\n",
        "plt.savefig('KMeans_distortions_graph.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eAoiC7wMSd75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quote from : https://github.com/smazzanti/are_you_still_using_elbow_method/blob/main/are-you-still-using-elbow-method.ipynb"
      ],
      "metadata": {
        "id": "e_tjbdnZT_4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as sklearn_metrics\n",
        "\n",
        "def calinski_harabasz_score(X, labels):\n",
        "  \"\"\"Wrapper function of Scikit-learn's calinski_harabasz_score. The only difference is it doesn't throw an error where there is only one label.\"\"\"\n",
        "  \n",
        "  if len(set(labels)) == 1:\n",
        "    return float(\"NaN\")\n",
        "  else:\n",
        "    return sklearn_metrics.calinski_harabasz_score(X, labels)\n",
        "\n",
        "\n",
        "def bic_score(X: np.ndarray, labels: np.array):\n",
        "  \"\"\"\n",
        "  BIC score for the goodness of fit of clusters.\n",
        "  This Python function is translated from the Golang implementation by the author of the paper. \n",
        "  The original code is available here: https://github.com/bobhancock/goxmeans/blob/a78e909e374c6f97ddd04a239658c7c5b7365e5c/km.go#L778\n",
        "  \"\"\"\n",
        "    \n",
        "  n_points = len(labels)\n",
        "  n_clusters = len(set(labels))\n",
        "  n_dimensions = X.shape[1]\n",
        "\n",
        "  n_parameters = (n_clusters - 1) + (n_dimensions * n_clusters) + 1\n",
        "\n",
        "  loglikelihood = 0\n",
        "  for label_name in set(labels):\n",
        "    X_cluster = X[labels == label_name]\n",
        "    n_points_cluster = len(X_cluster)\n",
        "    centroid = np.mean(X_cluster, axis=0)\n",
        "    variance = np.sum((X_cluster - centroid) ** 2) / (len(X_cluster) - 1)\n",
        "    loglikelihood += \\\n",
        "      n_points_cluster * np.log(n_points_cluster) \\\n",
        "      - n_points_cluster * np.log(n_points) \\\n",
        "      - n_points_cluster * n_dimensions / 2 * np.log(2 * math.pi * variance) \\\n",
        "      - (n_points_cluster - 1) / 2\n",
        "    \n",
        "  bic = loglikelihood - (n_parameters / 2) * np.log(n_points)\n",
        "        \n",
        "  return bic"
      ],
      "metadata": {
        "id": "fp6bJc7CUVTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training part\n",
        "n_clusters = 4\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "kmeans.fit(X)"
      ],
      "metadata": {
        "id": "RgQv_vKbVFKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User case\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# load data\n",
        "df = pd.read_csv('活跃用户分析.csv')\n",
        "\n",
        "# substract\n",
        "x = df[['年龄指数','参加活动指数','入会时间指数']].iloc[:,:].values\n",
        "\n",
        "# Choose the best K\n",
        "K = [1,2,3,4,5,6,7,8]\n",
        "GSSE = []\n",
        "for k in K:\n",
        "    SSE = []\n",
        "    kmeans = KMeans(n_clusters = k, random_state = 10)\n",
        "    kmeans.fit(x)\n",
        "    labels = kmeans.labels_\n",
        "    centers = kmeans.cluster_centers_\n",
        "    for label in set(labels):\n",
        "        SSE.append(np.sum(np.sum(x[labels == label,]-centers[label,:]**2)))\n",
        "    GSSE.append(np.sum(SSE))\n",
        "    \n",
        "# Plot the relationship between K and SSE(sum of squared errors)\n",
        "plt.plot(K,GSSE,\"b*-\")\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"SSE\")\n",
        "plt.title(\"Choose the best K\")\n",
        "plt.show()\n",
        "    \n",
        "# Define the model \n",
        "model = KMeans(n_clusters = 5)\n",
        "# Model fit\n",
        "model.fit(x)\n",
        "# Assign a cluster to each instance\n",
        "yhat = model.predict(x)\n",
        "# Create a sample scatter graph\n",
        "# plt.scatter(x[:,0],x[:,1],c = yhat,s= 50,cmap = 'viridis')\n",
        "\n",
        "# plot 3D clustering results\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "ax = plt.subplot(111, projection='3d')  # Create a 3D drawing\n",
        "# Divide the data points into three parts for drawing, with distinction in color\n",
        "ax.scatter(x[:,0],x[:,1],x[:,2],c = yhat,s= 5,cmap = 'viridis')\n",
        "centers = model.cluster_centers_\n",
        "\n",
        "# Create a scatter graph of clustering results\n",
        "ax.scatter(centers[:,0],centers[:,1],centers[:,2],c='red',s=100,alpha=0.5)\n",
        "\n",
        "# Plot\n",
        "pyplot.show()\n"
      ],
      "metadata": {
        "id": "4MT_XErMV-wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification tasks"
      ],
      "metadata": {
        "id": "JzJE_c5idXMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-layer classifier (neural network)"
      ],
      "metadata": {
        "id": "DrgURB4ZfOUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 50,50 is the pair which gets best result \n",
        "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(50,),max_iter = 50,verbose = True,random_state=1)\n",
        "clf.fit(X, y)\n",
        "# Model score\n",
        "r = clf.score(X, y)\n",
        "print(\"Accuracy:\", r)\n",
        "# Prediction\n",
        "y_predict = clf.predict(test_df.fillna(0))  \n",
        "y_predict = pd.DataFrame(y_predict, columns=['label'])\n",
        "y_predict.to_csv(\"NN.csv\", index=True, index_label='Id')"
      ],
      "metadata": {
        "id": "H7fOfLNbdl5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning the model\n",
        "parameter = {\"hidden_layer_sizes\": range(30,100,10),\n",
        "              \"max_iter\": range(30,120,10)}\n",
        "NN_model = MLPClassifier(verbose = True,solver= 'adam')\n",
        "clf = RandomizedSearchCV(NN_model, parameter) # Can change the cv algorithm here, but randomizedsearchcv is faster here\n",
        "clf.fit(X, y)\n",
        "print(clf.best_params_)"
      ],
      "metadata": {
        "id": "rTycmPeOfo_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model score\n",
        "r = clf.score(X, y)\n",
        "print(\"Accuracy:\", r)\n",
        "# Prediction\n",
        "y_predict = clf.predict(test_df.fillna(0))  \n",
        "y_predict = pd.DataFrame(y_predict, columns=['label'])\n",
        "y_predict.to_csv(\"NN-1.csv\", index=True, index_label='Id')"
      ],
      "metadata": {
        "id": "K_jcfAjTfsT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the result of GridSearchCV(if used)\n",
        "pd.DataFrame(clf.cv_results_).sort_values('rank_test_score').head(10)"
      ],
      "metadata": {
        "id": "0b4kLwBMf42j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validation\n",
        "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(50,),max_iter= 120,verbose = True,random_state=1)\n",
        "clf.fit(X_train, y_train)\n",
        "# Model score\n",
        "y_pred = clf.predict(X_test.fillna(0)) \n",
        "print(\"f1 score macro is :\",f1_score(y_test, y_pred, average='macro'))\n",
        "print(\"f1 score micro is :\",f1_score(y_test, y_pred, average='micro'))\n",
        "print(\"f1 score weighted is :\",f1_score(y_test, y_pred, average='weighted'))"
      ],
      "metadata": {
        "id": "f8adk20AgBzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost"
      ],
      "metadata": {
        "id": "bTAsdo-IgSXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost --upgrade\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "qT-6wbL1gVe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameter = {'n_estimators': [50, 100, 150]} \n",
        "             'learning_rate' : [0.05, 0.15, 0.30],\n",
        "             'max_depth' : [5, 10, 20],\n",
        "             'min_child_weight' : [ 1, 3, 7],\n",
        "             'gamma': [0.0, 0.2, 0.4],\n",
        "             'colsample_bytree' : [0.3, 0.5, 0.7]}"
      ],
      "metadata": {
        "id": "TSDy8vEtgWKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = xgb.XGBClassifier(n_jobs=1)\n",
        "clf = GridSearchCV(xgb_model, parameter)\n",
        "clf.fit(X, y)\n",
        "print(clf.best_params_)"
      ],
      "metadata": {
        "id": "4NjWdU8GkpeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(clf.cv_results_).sort_values('rank_test_score').head(10)"
      ],
      "metadata": {
        "id": "mw2YwYc9ksPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y = clf.predict(test_df.fillna(0))\n",
        "pred_df = pd.DataFrame(pred_y, columns=['label'])\n",
        "pred_df.to_csv(\"xgb.csv\", index=True, index_label='Id')"
      ],
      "metadata": {
        "id": "6gRZ8ZD6ku7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CatBoost"
      ],
      "metadata": {
        "id": "cFZWzwWlk27y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "from catboost import CatBoostRegressor"
      ],
      "metadata": {
        "id": "fGuoZuf-k6Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the index of categorical features\n",
        "cat_fea_idx = np.where(X.dtypes != np.float)[0]\n",
        "cat_model = CatBoostClassifier(iterations=1000 ,\n",
        "                               depth=4,\n",
        "                               od_type=\"Iter\",\n",
        "                               early_stopping_rounds=500,\n",
        "                               loss_function='MultiClass' )\n",
        "cat_model.fit(X, y)\n",
        "pred_y = cat_model.predict(test_df.fillna(0))\n",
        "pred_df = pd.DataFrame(pred_y, columns=['label'])\n",
        "pred_df.to_csv(\"cat.csv\", index=True, index_label='Id')"
      ],
      "metadata": {
        "id": "VfVHq5j_k62x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression Tasks"
      ],
      "metadata": {
        "id": "kaZVzUr-lmop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LightGBM(Model Interpretation)"
      ],
      "metadata": {
        "id": "1t1H5Xx2lqPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Interpretaion\n",
        "! pip install shap"
      ],
      "metadata": {
        "id": "rkEt6KqimC0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "shap.initjs()"
      ],
      "metadata": {
        "id": "ka9MMQW9miIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiation of a LGBMRegressor model\n",
        "\n",
        "lgbm_reg = LGBMRegressor(n_estimators = 200, learning_rate = 0.07, n_jobs = -1, random_state = 42)\n",
        "lgbm_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict log_SalePrice for the variables in the training set\n",
        "y_pred_lgbm_train = lgbm_reg.predict(X_train_scaled)\n",
        "\n",
        "rmse_lgbm_train = np.sqrt(metrics.mean_squared_error(y_train, y_pred_lgbm_train))\n",
        "r2_lgbm_train = metrics.r2_score(y_train, y_pred_lgbm_train)\n",
        "print(\"Training RMSE:\", rmse_lgbm_train)\n",
        "print(\"Training R²:\", r2_lgbm_train)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Predict log_SalePrice for the variables in the validation set\n",
        "y_pred_lgbm = lgbm_reg.predict(X_val_scaled)\n",
        "\n",
        "rmse_lgbm = np.sqrt(metrics.mean_squared_error(y_val, y_pred_lgbm))\n",
        "r2_lgbm = metrics.r2_score(y_val, y_pred_lgbm)\n",
        "print(\"Out-of-sample RMSE:\", rmse_lgbm)\n",
        "print(\"Out-of-sample R²:\", r2_lgbm)"
      ],
      "metadata": {
        "id": "ChI5W1k7mvmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model result for later use\n",
        "dump(lgbm_reg, 'lgbm_reg.joblib')"
      ],
      "metadata": {
        "id": "o7uiNTtnm5Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model\n",
        "loaded_model = load('lgbm_reg.joblib')"
      ],
      "metadata": {
        "id": "i-UPgJo_m-hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot one of the decision tree to see how the criteria work\n",
        "lightgbm.plot_tree(lgbm_reg,tree_index=0,figsize=(100,50))"
      ],
      "metadata": {
        "id": "DlHTR20cnIAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_labels = ['', '', '']"
      ],
      "metadata": {
        "id": "MOyBaJ_SnMph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to use SHAP value to interprete the model\n",
        "explainer = shap.Explainer(lgbm_reg)\n",
        "shap_values = explainer(X_train_scaled)\n",
        "\n",
        "shap.summary_plot(shap_values, X_train_scaled, plot_type = \"bar\",feature_names=custom_labels)\n",
        "shap.plots.bar(shap_values)"
      ],
      "metadata": {
        "id": "C7OQXmW0narr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The impact of the feature on model output\n",
        "shap.summary_plot(shap_values, X_train_scaled,feature_names=custom_labels)"
      ],
      "metadata": {
        "id": "mfH8XJNznbXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the heatmap for each instance\n",
        "shap_values_explaination = shap.Explanation(shap_values, feature_names=custom_labels) \n",
        "shap.plots.heatmap(shap_values_explaination)"
      ],
      "metadata": {
        "id": "yIsXDmBwndrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the column name\n",
        "shap_values.feature_names = custom_labels"
      ],
      "metadata": {
        "id": "eBElrwq8nlNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse the scaler\n",
        "shap_values.data = scaler.inverse_transform(shap_values.data)"
      ],
      "metadata": {
        "id": "gTWRZdd4nnVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot only one instance\n",
        "shap.plots.waterfall(shap_values[0])"
      ],
      "metadata": {
        "id": "za4tRVcZnqti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation"
      ],
      "metadata": {
        "id": "s8vlxyWZfao8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting ROC Curve\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred_prob = logreg.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.title('ROC curve for xx classifier')\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "dkfcop5MdcMS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}